{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from tabulate import tabulate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_from_path(path):\n",
    "    \"\"\"\n",
    "    Extracts dataset name, author, context policy and model from the given path.\n",
    "    \"\"\"\n",
    "    parts = path.split('/')\n",
    "    name_parts = parts[-1].split('_')\n",
    "    dataset_name = name_parts[0].replace('-',' ').capitalize()\n",
    "    author = name_parts[1].capitalize() + \" \" + name_parts[2].capitalize()\n",
    "    context_policy = name_parts[3].replace('-', ' ').capitalize()\n",
    "    model = name_parts[4].replace('-',' ').replace('.csv','').capitalize()\n",
    "    return dataset_name, author, model, context_policy\n",
    "\n",
    "def compute_roc_values(auth1_df, auth2_df):\n",
    "    \"\"\"\n",
    "    input: auth1_df, auth2_df holding response values over the same LM\n",
    "    :return: ROC values where labels are 1 for auth1 and 0 for auth2, threshold is perplexity value and\n",
    "    TP are all author1 responses with perplexity value above threshold\n",
    "    FP are all author2 responses with perplexity value above threshold\n",
    "    TN are all author2 responses with perplexity value below threshold\n",
    "    FN are all author1 responses with perplexity value below threshold\n",
    "    \"\"\"\n",
    "    labels = np.concatenate([np.zeros(len(auth1_df)), np.ones(len(auth2_df))])\n",
    "    responses = np.concatenate([auth1_df['response'], auth2_df['response']])\n",
    "\n",
    "    # Handle NaN values\n",
    "    nan_mask = np.isnan(responses)\n",
    "    labels = labels[~nan_mask]\n",
    "    responses = responses[~nan_mask]\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(labels, responses)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "def calc_diff(auth1_path, auth2_path):\n",
    "    \"\"\"\n",
    "    input: paths of author1 and author2 csv's holding responses for each sentence over the same LM\n",
    "    output: Standardized Mean Difference between the authors responses (author1_log-ppx - author2_log-ppx)/pooled_std\n",
    "    \"\"\"\n",
    "    auth1_df, auth2_df = pd.read_csv(auth1_path), pd.read_csv(auth2_path)\n",
    "    auth1_mean, auth2_mean = auth1_df[\"response\"].mean(), auth2_df[\"response\"].mean()\n",
    "    auth1_std, auth2_std = auth1_df[\"response\"].std(), auth2_df[\"response\"].std()\n",
    "    len_auth1, len_auth2 = len(auth1_df), len(auth2_df)\n",
    "    pooled_std = np.sqrt(((len_auth1-1) * auth1_std**2 + (len_auth2-1) * auth2_std**2)/ (len_auth1 + len_auth2 -2))\n",
    "    diff = abs((auth1_mean - auth2_mean)/pooled_std)\n",
    "    return diff\n",
    "\n",
    "def lm_aggregate_metrics(lm_paths):\n",
    "    \"\"\"\n",
    "    Compute AUC and standardized mean difference (calc_diff) for two use-cases:\n",
    "    1. LM separator is one of the authors.\n",
    "    2. LM separator is not one of the authors.\n",
    "    \n",
    "    :param paths: List paths containing the LM repsonses values over all authors\n",
    "    :return: Results for both groups for each LM.\n",
    "    \"\"\"\n",
    "\n",
    "    _, _, lm_name, _ = extract_info_from_path(lm_paths[0])\n",
    "    group1_aucs, group1_diffs = [], [] # LM in authors\n",
    "    group2_aucs, group2_diffs = [], [] # LM not in authors\n",
    "\n",
    "    for i, auth1_path in enumerate(lm_paths):       # Iterate over authors\n",
    "        for j, auth2_path in enumerate(lm_paths):\n",
    "            if i >= j:                              # Avoid self-comparisons or duplicate pairs\n",
    "                continue\n",
    "\n",
    "            _, author1, _, _ = extract_info_from_path(auth1_path)\n",
    "            _, author2, _, _ = extract_info_from_path(auth2_path)\n",
    "\n",
    "            auth1_df = pd.read_csv(auth1_path)\n",
    "            auth2_df = pd.read_csv(auth2_path)\n",
    "            _, _, roc_auc = compute_roc_values(auth1_df, auth2_df)\n",
    "            diff = calc_diff(auth1_path, auth2_path)\n",
    "\n",
    "            # print(f\"LM Name: {lm_name}, Author1: {author1}, Author2: {author2}\")\n",
    "            # print(auth1_df[\"response\"].isna().sum(), auth2_df[\"response\"].isna().sum())\n",
    "            # plt.hist(auth1_df[\"response\"], bins=50, alpha=0.5, label=\"Author1\")\n",
    "            # plt.hist(auth2_df[\"response\"], bins=50, alpha=0.5, label=\"Author2\")\n",
    "            # plt.legend()\n",
    "            # plt.show()\n",
    "\n",
    "            # Check if the LM matches any part of the author names\n",
    "            if any(keyword in author.lower() for keyword in lm_name.lower().split() for author in [author1, author2]):\n",
    "                group1_aucs.append(roc_auc)\n",
    "                group1_diffs.append(diff)\n",
    "            else:\n",
    "                group2_aucs.append(roc_auc)\n",
    "                group2_diffs.append(diff)\n",
    "\n",
    "    return group1_aucs, group1_diffs, group2_aucs, group2_diffs\n",
    "\n",
    "def lm_comparison_table(group1_aucs, group1_diffs, group2_aucs, group2_diffs, lm_name):\n",
    "    \"\"\" table preparation for per LM separator \"\"\"\n",
    "    avg_auc_in_authors = np.mean(group1_aucs) if group1_aucs else 0\n",
    "    avg_diff_in_authors = np.mean(group1_diffs) if group1_diffs else 0\n",
    "    avg_auc_not_in_authors = np.mean(group2_aucs) if group2_aucs else 0\n",
    "    avg_diff_not_in_authors = np.mean(group2_diffs) if group2_diffs else 0\n",
    "    return [lm_name, avg_auc_in_authors, avg_diff_in_authors, avg_auc_not_in_authors, avg_diff_not_in_authors]\n",
    "\n",
    "def all_lms_comparison_table(group1_aucs, group1_diffs, group2_aucs, group2_diffs):\n",
    "    \"\"\" table preparation for aggregate over all LMs \"\"\"\n",
    "    avg_auc_in_authors = np.mean(group1_aucs) if group1_aucs else 0\n",
    "    avg_diff_in_authors = np.mean(group1_diffs) if group1_diffs else 0\n",
    "    avg_auc_not_in_authors = np.mean(group2_aucs) if group2_aucs else 0\n",
    "    avg_diff_not_in_authors = np.mean(group2_diffs) if group2_diffs else 0\n",
    "    return [\"All LMs\", avg_auc_in_authors, avg_diff_in_authors, avg_auc_not_in_authors, avg_diff_not_in_authors]\n",
    "\n",
    "def compare_hist(auth1_path1, auth2_path1, auth1_path2, auth2_path2):\n",
    "    \"\"\"\n",
    "    input: paths of author1 and author2 csv holding responses for each sentence\n",
    "    output: histograms of author1 and author2 log-ppx values\n",
    "    \"\"\"\n",
    "    auth1_df1 = pd.read_csv(auth1_path1)\n",
    "    auth2_df1 = pd.read_csv(auth2_path1)\n",
    "    auth1_df2 = pd.read_csv(auth1_path2)\n",
    "    auth2_df2 = pd.read_csv(auth2_path2)\n",
    "\n",
    "    dataset_name, author1, model1, _ = extract_info_from_path(auth1_path1)\n",
    "    _, _, model2, _ = extract_info_from_path(auth1_path2)\n",
    "    _, author2, _, _ = extract_info_from_path(auth2_path2)\n",
    "\n",
    "\n",
    "    # Compute ROC values for each dataset\n",
    "    fpr1, tpr1, roc_auc1 = compute_roc_values(auth1_df1, auth2_df1)\n",
    "    fpr2, tpr2, roc_auc2 = compute_roc_values(auth1_df2, auth2_df2)\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    bins = np.arange(min(auth1_df1[\"response\"].min(), auth2_df1[\"response\"].min(),auth1_df2[\"response\"].min(), auth2_df2[\"response\"].min()),\n",
    "                max(auth1_df1[\"response\"].max(), auth2_df1[\"response\"].max(), auth1_df2[\"response\"].max(), auth2_df2[\"response\"].max()),\n",
    "                     0.1)\n",
    "    axs[0, 0].hist(auth1_df1[\"response\"], bins=bins, alpha=0.5, label=author1)\n",
    "    axs[0, 0].hist(auth2_df1[\"response\"], bins=bins, alpha=0.5, label=author2)\n",
    "    axs[0, 0].set_title(f\"LM response generator - {model1}\")\n",
    "    axs[0, 0].set_xlabel('Log-perplexity')\n",
    "    axs[0, 0].set_ylabel('Frequency')\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    axs[0, 1].hist(auth1_df2[\"response\"], bins=bins, alpha=0.5, label=author1)\n",
    "    axs[0, 1].hist(auth2_df2[\"response\"], bins=bins, alpha=0.5, label=author2)\n",
    "    axs[0, 1].set_title(f\"LM response generator - {model2}\")\n",
    "    axs[0, 1].set_xlabel('Log-perplexity')\n",
    "    axs[0, 1].set_ylabel('Frequency')\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    # Plot the ROC curves using the computed values\n",
    "    axs[1, 0].plot(fpr1, tpr1, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc1:.4f})')\n",
    "    axs[1, 0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    axs[1, 0].set_xlabel('False Positive Rate')\n",
    "    axs[1, 0].set_ylabel('True Positive Rate')\n",
    "    axs[1, 0].legend(loc='lower right')\n",
    "    axs[1, 0].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    axs[1, 1].plot(fpr2, tpr2, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc2:.4f})')\n",
    "    axs[1, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    axs[1, 1].set_xlabel('False Positive Rate')\n",
    "    axs[1, 1].set_ylabel('True Positive Rate')\n",
    "    axs[1, 1].legend(loc='lower right')\n",
    "    axs[1, 1].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    plt.suptitle(f\"Dataset - {dataset_name}\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# def calc_mean_ppx_instance(author1_responses, author2_responses):\n",
    "#     \"\"\"\n",
    "#     Each instance in the dataset has a unique \"name\" value with a different number of sentences for author responses\n",
    "#     This method calculates the mean perplexity for each instance\n",
    "#     :param author1_responses: df with responses\n",
    "#     :param author2_responses: df with responses\n",
    "#     :return: dataframes containing the mean perplexity for each instance\n",
    "#     \"\"\"\n",
    "#     author1_grouped_mean = author1_responses.groupby('name')['response'].mean().reset_index()\n",
    "#     author1_sorted_df = author1_grouped_mean.sort_values(by='name', ascending=True)\n",
    "#     author2_grouped_mean = author2_responses.groupby('name')['response'].mean().reset_index()\n",
    "#     author2_sorted_df = author2_grouped_mean.sort_values(by='name', ascending=True)\n",
    "#     return author1_sorted_df, author2_sorted_df\n",
    "\n",
    "# def calc_diff_ppx_instance(author1_responses, author2_responses):\n",
    "#     \"\"\"\n",
    "#     input: author1/author2 mean responses sorted my 'name' (samples)\n",
    "#     returns df holding name and the difference between author1 to author2 responses\n",
    "#     Note: the larger the difference the 'better' in seperation\n",
    "#     \"\"\"\n",
    "#     diff_df = author1_responses.copy()\n",
    "#     diff_df[\"response\"] = author1_responses[\"response\"] - author2_responses[\"response\"]\n",
    "#     return diff_df\n",
    "\n",
    "# def compare_lm_results(auth1_path_base, auth2_path_base, auth1_path, auth2_path):\n",
    "#     \"\"\"\n",
    "#     Compares responses from a baseline LM to responses from an additional LM.\n",
    "\n",
    "#     input: \n",
    "#         - auth1_path_base, auth2_path_base: Paths to baseline LM responses for Author1 and Author2.\n",
    "#         - auth1_path, auth2_path: Paths to comparison LM responses for Author1 and Author2.\n",
    "#     output: \n",
    "#         - List of response results containing: \n",
    "#             [Author1 mean response, Author2 mean response, Auth1-Auth2 log-ppx difference, \n",
    "#     \"\"\"\n",
    "#     auth1_df, auth2_df = pd.read_csv(auth1_path), pd.read_csv(auth2_path)\n",
    "#     auth1_mean, auth2_mean = auth1_df[\"response\"].mean(), auth2_df[\"response\"].mean()\n",
    "#     diff = calc_diff(auth1_path, auth2_path)\n",
    "#     base_diff = calc_diff(auth1_path_base, auth2_path_base)\n",
    "#     diff_from_base = round((diff - base_diff),4)\n",
    "#     if diff_from_base > 0:\n",
    "#         diff_from_base = f\"+ {diff_from_base} ↑\"\n",
    "#     elif diff_from_base < 0:\n",
    "#         diff_from_base =  f\"- {-diff_from_base} ↓\"\n",
    "#     else:\n",
    "#         diff_from_base = \"0\"\n",
    "\n",
    "#     _, _, roc_auc = compute_roc_values(auth1_df, auth2_df)\n",
    "\n",
    "#     return [auth1_mean, auth2_mean, diff, diff_from_base, roc_auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to csv's holding response values and sentence lengths\n",
    "wiki_paths = [[\"Responses/wiki_Llama3.1_clean_none_Meta-Llama-3.1-8B-Instruct.csv\", \"Responses/wiki_Falcon_clean_none_Meta-Llama-3.1-8B-Instruct.csv\", \"Responses/wiki_human_text_none_Meta-Llama-3.1-8B-Instruct.csv\"],\n",
    "              [\"Responses/wiki_Llama3.1_clean_none_falcon-7b.csv\",\"Responses/wiki_Falcon_clean_none_falcon-7b.csv\",\"Responses/wiki_human_text_none_falcon-7b.csv\"],\n",
    "              [\"Responses/wiki_Llama3.1_clean_none_phi-2.csv\",\"Responses/wiki_Falcon_clean_none_phi-2.csv\",\"Responses/wiki_human_text_none_phi-2.csv\"]]\n",
    "\n",
    "news_paths = [[\"Responses/news_Llama3.1_clean_none_Meta-Llama-3.1-8B-Instruct.csv\", \"Responses/news_Falcon_clean_none_Meta-Llama-3.1-8B-Instruct.csv\", \"Responses/news_human_text_none_Meta-Llama-3.1-8B-Instruct.csv\"],\n",
    "              [\"Responses/news_Llama3.1_clean_none_falcon-7b.csv\",\"Responses/news_Falcon_clean_none_falcon-7b.csv\",\"Responses/news_human_text_none_falcon-7b.csv\"],\n",
    "              [\"Responses/news_Llama3.1_clean_none_phi-2.csv\",\"Responses/news_Falcon_clean_none_phi-2.csv\",\"Responses/news_human_text_none_phi-2.csv\"]]\n",
    "\n",
    "abstracts_paths = [[\"Responses/abstracts_Llama3.1_clean_none_Meta-Llama-3.1-8B-Instruct.csv\", \"Responses/abstracts_Falcon_clean_none_Meta-Llama-3.1-8B-Instruct.csv\", \"Responses/abstracts_human_text_none_Meta-Llama-3.1-8B-Instruct.csv\"],\n",
    "              [\"Responses/abstracts_Llama3.1_clean_none_falcon-7b.csv\",\"Responses/abstracts_Falcon_clean_none_falcon-7b.csv\",\"Responses/abstracts_human_text_none_falcon-7b.csv\"],\n",
    "              [\"Responses/abstracts_Llama3.1_clean_none_phi-2.csv\",\"Responses/abstracts_Falcon_clean_none_phi-2.csv\",\"Responses/abstracts_human_text_none_phi-2.csv\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Wiki Dataset ===\n",
      "\n",
      "Results for Meta llama 3.1 8b instruct separator:\n",
      "LM-In-Authors:\n",
      "    AUCs: [0.7471274732960242, 0.8642764178741291]\n",
      "    Diffs: [0.9553828959238608, 1.3778681585758543]\n",
      "LM-Not-In-Authors:\n",
      "    AUCs: [0.5448500220720822]\n",
      "    Diffs: [0.29174480358125343]\n",
      "\n",
      "Results for Falcon 7b separator:\n",
      "LM-In-Authors:\n",
      "    AUCs: [0.5407859603180524, 0.6216545463316389]\n",
      "    Diffs: [0.4646867388783228, 0.19062677338740236]\n",
      "LM-Not-In-Authors:\n",
      "    AUCs: [0.7715598124812962]\n",
      "    Diffs: [1.0437651074235406]\n",
      "\n",
      "Results for Phi 2 separator:\n",
      "LM-In-Authors:\n",
      "    AUCs: []\n",
      "    Diffs: []\n",
      "LM-Not-In-Authors:\n",
      "    AUCs: [0.6836347739656425, 0.8873129819026562, 0.6558719387187516]\n",
      "    Diffs: [0.7384709643308915, 1.6166164499858928, 0.2265489091456213]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Wiki Dataset ===\")\n",
    "group1_aucs, group1_diffs, group2_aucs, group2_diffs = [],[],[],[]\n",
    "for i, lm_paths in enumerate(wiki_paths):\n",
    "    lm_is_author_aucs, lm_is_author_diffs, lm_not_author_aucs, lm_not_author_diffs = lm_aggregate_metrics(lm_paths)\n",
    "    group1_aucs.extend(lm_is_author_aucs)\n",
    "    group1_diffs.extend(lm_is_author_diffs)\n",
    "    group2_aucs.extend(lm_not_author_aucs)\n",
    "    group2_diffs.extend(lm_not_author_diffs)\n",
    "    _, _, lm_name, _ = extract_info_from_path(lm_paths[0])\n",
    "\n",
    "    print(f\"\\nResults for {lm_name} separator:\")\n",
    "    print(f\"LM-In-Authors:\")\n",
    "    print(f\"    AUCs: {lm_is_author_aucs}\")\n",
    "    print(f\"    Diffs: {lm_is_author_diffs}\")\n",
    "    print(f\"LM-Not-In-Authors:\")\n",
    "    print(f\"    AUCs: {lm_not_author_aucs}\")\n",
    "    print(f\"    Diffs: {lm_not_author_diffs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Wiki Dataset ===\n",
      "\n",
      "Per-LM Comparison Table\n",
      "+----------------------------+---------------------------+----------------------------+-------------------------------+--------------------------------+\n",
      "| LM Separator               |   Avg AUC (LM in Authors) |   Avg Diff (LM in Authors) |   Avg AUC (LM not in Authors) |   Avg Diff (LM not in Authors) |\n",
      "|----------------------------+---------------------------+----------------------------+-------------------------------+--------------------------------|\n",
      "| Meta llama 3.1 8b instruct |                  0.805702 |                   1.16663  |                      0.54485  |                       0.291745 |\n",
      "| Falcon 7b                  |                  0.58122  |                   0.327657 |                      0.77156  |                       1.04377  |\n",
      "| Phi 2                      |                  0        |                   0        |                      0.742273 |                       0.860545 |\n",
      "+----------------------------+---------------------------+----------------------------+-------------------------------+--------------------------------+\n",
      "\n",
      "All LMs Aggregate Table\n",
      "+----------------+---------------------------+----------------------------+-------------------------------+--------------------------------+\n",
      "| LM Separator   |   Avg AUC (LM in Authors) |   Avg Diff (LM in Authors) |   Avg AUC (LM not in Authors) |   Avg Diff (LM not in Authors) |\n",
      "|----------------+---------------------------+----------------------------+-------------------------------+--------------------------------|\n",
      "| All LMs        |                  0.693461 |                   0.747141 |                      0.708646 |                       0.783429 |\n",
      "+----------------+---------------------------+----------------------------+-------------------------------+--------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Results for Wiki dataset\n",
    "print(\"=== Wiki Dataset ===\")\n",
    "lm_comparison_results = []\n",
    "group1_aucs_all, group1_diffs_all, group2_aucs_all, group2_diffs_all = [], [], [], []\n",
    "\n",
    "for i, lm_paths in enumerate(wiki_paths):\n",
    "    lm_is_author_aucs, lm_is_author_diffs, lm_not_author_aucs, lm_not_author_diffs = lm_aggregate_metrics(lm_paths)\n",
    "    group1_aucs_all.extend(lm_is_author_aucs)\n",
    "    group1_diffs_all.extend(lm_is_author_diffs)\n",
    "    group2_aucs_all.extend(lm_not_author_aucs)\n",
    "    group2_diffs_all.extend(lm_not_author_diffs)\n",
    "    _, _, lm_name, _ = extract_info_from_path(lm_paths[0])\n",
    "\n",
    "    lm_comparison_results.append(lm_comparison_table(lm_is_author_aucs, lm_is_author_diffs, lm_not_author_aucs, lm_not_author_diffs, lm_name))\n",
    "\n",
    "# first table (per LM separator)\n",
    "columns = [\"LM Separator\", \"Avg AUC (LM in Authors)\", \"Avg Diff (LM in Authors)\", \"Avg AUC (LM not in Authors)\", \"Avg Diff (LM not in Authors)\"]\n",
    "print(\"\\nPer-LM Comparison Table\")\n",
    "print(tabulate(lm_comparison_results, headers=columns, tablefmt=\"psql\"))\n",
    "\n",
    "# second table (aggregate over all LMs)\n",
    "all_lms_table = all_lms_comparison_table(group1_aucs_all, group1_diffs_all, group2_aucs_all, group2_diffs_all)\n",
    "print(\"\\nAll LMs Aggregate Table\")\n",
    "print(tabulate([all_lms_table], headers=columns, tablefmt=\"psql\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== News Dataset ===\n",
      "\n",
      "Per-LM Comparison Table\n",
      "+----------------------------+---------------------------+----------------------------+-------------------------------+--------------------------------+\n",
      "| LM Separator               |   Avg AUC (LM in Authors) |   Avg Diff (LM in Authors) |   Avg AUC (LM not in Authors) |   Avg Diff (LM not in Authors) |\n",
      "|----------------------------+---------------------------+----------------------------+-------------------------------+--------------------------------|\n",
      "| Meta llama 3.1 8b instruct |                  0.801406 |                   0.891757 |                      0.593834 |                      0.0276665 |\n",
      "| Falcon 7b                  |                  0.625791 |                   0.452911 |                      0.809437 |                      1.01977   |\n",
      "| Phi 2                      |                  0        |                   0        |                      0.744389 |                      0.676006  |\n",
      "+----------------------------+---------------------------+----------------------------+-------------------------------+--------------------------------+\n",
      "\n",
      "All LMs Aggregate Table\n",
      "+----------------+---------------------------+----------------------------+-------------------------------+--------------------------------+\n",
      "| LM Separator   |   Avg AUC (LM in Authors) |   Avg Diff (LM in Authors) |   Avg AUC (LM not in Authors) |   Avg Diff (LM not in Authors) |\n",
      "|----------------+---------------------------+----------------------------+-------------------------------+--------------------------------|\n",
      "| All LMs        |                  0.713599 |                   0.672334 |                      0.727287 |                        0.61509 |\n",
      "+----------------+---------------------------+----------------------------+-------------------------------+--------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Results for news dataset\n",
    "print(\"=== News Dataset ===\")\n",
    "lm_comparison_results = []\n",
    "group1_aucs_all, group1_diffs_all, group2_aucs_all, group2_diffs_all = [], [], [], []\n",
    "\n",
    "for i, lm_paths in enumerate(news_paths):\n",
    "    lm_is_author_aucs, lm_is_author_diffs, lm_not_author_aucs, lm_not_author_diffs = lm_aggregate_metrics(lm_paths)\n",
    "    group1_aucs_all.extend(lm_is_author_aucs)\n",
    "    group1_diffs_all.extend(lm_is_author_diffs)\n",
    "    group2_aucs_all.extend(lm_not_author_aucs)\n",
    "    group2_diffs_all.extend(lm_not_author_diffs)\n",
    "    _, _, lm_name, _ = extract_info_from_path(lm_paths[0])\n",
    "\n",
    "    lm_comparison_results.append(lm_comparison_table(lm_is_author_aucs, lm_is_author_diffs, lm_not_author_aucs, lm_not_author_diffs, lm_name))\n",
    "\n",
    "# first table (per LM separator)\n",
    "columns = [\"LM Separator\", \"Avg AUC (LM in Authors)\", \"Avg Diff (LM in Authors)\", \"Avg AUC (LM not in Authors)\", \"Avg Diff (LM not in Authors)\"]\n",
    "print(\"\\nPer-LM Comparison Table\")\n",
    "print(tabulate(lm_comparison_results, headers=columns, tablefmt=\"psql\"))\n",
    "\n",
    "# second table (aggregate over all LMs)\n",
    "all_lms_table = all_lms_comparison_table(group1_aucs_all, group1_diffs_all, group2_aucs_all, group2_diffs_all)\n",
    "print(\"\\nAll LMs Aggregate Table\")\n",
    "print(tabulate([all_lms_table], headers=columns, tablefmt=\"psql\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Abstracts Dataset ===\n",
      "\n",
      "Per-LM Comparison Table\n",
      "+----------------------------+---------------------------+----------------------------+-------------------------------+--------------------------------+\n",
      "| LM Separator               |   Avg AUC (LM in Authors) |   Avg Diff (LM in Authors) |   Avg AUC (LM not in Authors) |   Avg Diff (LM not in Authors) |\n",
      "|----------------------------+---------------------------+----------------------------+-------------------------------+--------------------------------|\n",
      "| Meta llama 3.1 8b instruct |                  0.711765 |                   0.724436 |                      0.650887 |                       0.230043 |\n",
      "| Falcon 7b                  |                  0.599284 |                   0.389243 |                      0.771363 |                       0.91704  |\n",
      "| Phi 2                      |                  0        |                   0        |                      0.718649 |                       0.779301 |\n",
      "+----------------------------+---------------------------+----------------------------+-------------------------------+--------------------------------+\n",
      "\n",
      "All LMs Aggregate Table\n",
      "+----------------+---------------------------+----------------------------+-------------------------------+--------------------------------+\n",
      "| LM Separator   |   Avg AUC (LM in Authors) |   Avg Diff (LM in Authors) |   Avg AUC (LM not in Authors) |   Avg Diff (LM not in Authors) |\n",
      "|----------------+---------------------------+----------------------------+-------------------------------+--------------------------------|\n",
      "| All LMs        |                  0.655525 |                    0.55684 |                       0.71564 |                       0.696997 |\n",
      "+----------------+---------------------------+----------------------------+-------------------------------+--------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Results for news dataset\n",
    "print(\"=== Abstracts Dataset ===\")\n",
    "lm_comparison_results = []\n",
    "group1_aucs_all, group1_diffs_all, group2_aucs_all, group2_diffs_all = [], [], [], []\n",
    "\n",
    "for i, lm_paths in enumerate(abstracts_paths):\n",
    "    lm_is_author_aucs, lm_is_author_diffs, lm_not_author_aucs, lm_not_author_diffs = lm_aggregate_metrics(lm_paths)\n",
    "    group1_aucs_all.extend(lm_is_author_aucs)\n",
    "    group1_diffs_all.extend(lm_is_author_diffs)\n",
    "    group2_aucs_all.extend(lm_not_author_aucs)\n",
    "    group2_diffs_all.extend(lm_not_author_diffs)\n",
    "    _, _, lm_name, _ = extract_info_from_path(lm_paths[0])\n",
    "\n",
    "    lm_comparison_results.append(lm_comparison_table(lm_is_author_aucs, lm_is_author_diffs, lm_not_author_aucs, lm_not_author_diffs, lm_name))\n",
    "\n",
    "# first table (per LM separator)\n",
    "columns = [\"LM Separator\", \"Avg AUC (LM in Authors)\", \"Avg Diff (LM in Authors)\", \"Avg AUC (LM not in Authors)\", \"Avg Diff (LM not in Authors)\"]\n",
    "print(\"\\nPer-LM Comparison Table\")\n",
    "print(tabulate(lm_comparison_results, headers=columns, tablefmt=\"psql\"))\n",
    "\n",
    "# second table (aggregate over all LMs)\n",
    "all_lms_table = all_lms_comparison_table(group1_aucs_all, group1_diffs_all, group2_aucs_all, group2_diffs_all)\n",
    "print(\"\\nAll LMs Aggregate Table\")\n",
    "print(tabulate([all_lms_table], headers=columns, tablefmt=\"psql\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
