{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch  \n",
    "# !pip install datasets  \n",
    "# !hostname\n",
    "# !pip install --upgrade transformers\n",
    "# !pip install spacy\n",
    "# !pip install ipywidgets\n",
    "# !pip install --upgrade transformers \"numpy<2\" \"pyarrow<14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# This notebook consists of two main parts:\n",
    "#   1. Loading of the datasets, populating article lengths, crating the prompts and saving to domain_dataset.csv\n",
    "#   2. Generating articles for each model in batches and save results incrementally to domain_dataset_out.csv\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec  7 19:17:49 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off |   00000001:00:00.0 Off |                    0 |\n",
      "| N/A   34C    P0             42W /  300W |       6MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# !nvcc --version\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/akp3user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import set_seed\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from PrepareSentenceContext import PrepareSentenceContext\n",
    "\n",
    "nltk.download('punkt')\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu124\n",
      "CUDA is available: True\n",
      "CUDA version: 12.4\n"
     ]
    }
   ],
   "source": [
    "# Check current PyTorch and CUDA availability\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # notebook_login()\n",
    "# from huggingface_hub import login\n",
    "# login(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "### Part 1 ###\n",
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'url', 'title', 'title_len', 'prompt', 'human_text', 'human_len',\n",
      "       'gpt', 'gpt_len', 'Llama3.1', 'Llama3.1_len', 'Falcon', 'Falcon_len'],\n",
      "      dtype='object')\n",
      "39495\n"
     ]
    }
   ],
   "source": [
    "# Load the HuggingFace wiki_intro_long dataset\n",
    "hf_wiki_dataset = load_dataset('alonkipnis/wiki-intro-long', split='train')\n",
    "df_wiki = pd.DataFrame(hf_wiki_dataset)\n",
    "\n",
    "# Add columns for Llama2 and Falcon7B model outputs\n",
    "df_wiki['human_len'] = None\n",
    "df_wiki['gpt_len'] = None\n",
    "df_wiki['Llama3.1'], df_wiki['Llama3.1_len'] = None, None\n",
    "df_wiki['Falcon'], df_wiki['Falcon_len'] = None, None\n",
    "\n",
    "df_wiki.rename(columns={\n",
    "    'wiki_intro': 'human_text',\n",
    "    'generated_intro': 'gpt'\n",
    "    }, inplace=True)\n",
    "\n",
    "columns_to_drop = ['prompt_tokens', 'generated_text', 'generated_intro_len']\n",
    "df_wiki.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "new_order = [\n",
    "    'id', 'url', 'title', 'title_len', 'prompt',\n",
    "    'human_text', 'human_len',\n",
    "    'gpt', 'gpt_len',\n",
    "    'Llama3.1', 'Llama3.1_len',\n",
    "    'Falcon', 'Falcon_len']\n",
    "\n",
    "df_wiki = df_wiki[new_order]\n",
    "print(df_wiki.columns)\n",
    "print(df_wiki.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'highlights', 'prompt', 'human_text', 'human_len', 'gpt',\n",
      "       'gpt_len', 'Llama3.1', 'Llama3.1_len', 'Falcon', 'Falcon_len'],\n",
      "      dtype='object')\n",
      "13025\n"
     ]
    }
   ],
   "source": [
    "# Load the HuggingFace news dataset\n",
    "hf_news_dataset = load_dataset('alonkipnis/news-chatgpt-long', split='train')\n",
    "df_news = pd.DataFrame(hf_news_dataset)\n",
    "\n",
    "df_news.rename(columns={\n",
    "    'article': 'human_text',\n",
    "    'chatgpt': 'gpt'\n",
    "}, inplace=True)\n",
    "\n",
    "df_news['human_len'], df_news['gpt_len'] = None, None\n",
    "df_news['Llama3.1'], df_news['Llama3.1_len'] = None, None\n",
    "df_news['Falcon'], df_news['Falcon_len'] = None, None\n",
    "df_news['prompt'] = None\n",
    "\n",
    "new_order = [\n",
    "    'id', 'highlights', 'prompt',\n",
    "    'human_text', 'human_len',\n",
    "    'gpt', 'gpt_len',\n",
    "    'Llama3.1', 'Llama3.1_len',\n",
    "    'Falcon', 'Falcon_len'\n",
    "]\n",
    "\n",
    "df_news = df_news[new_order]\n",
    "print(df_news.columns)\n",
    "print(df_news.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'prompt', 'human_text', 'human_len', 'gpt', 'gpt_len',\n",
      "       'Llama3.1', 'Llama3.1_len', 'Falcon', 'Falcon_len'],\n",
      "      dtype='object')\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Load the HuggingFace research absracts dataset\n",
    "hf_abstracts_dataset = load_dataset('NicolaiSivesind/ChatGPT-Research-Abstracts', split='train')\n",
    "df_abstracts = pd.DataFrame(hf_abstracts_dataset)\n",
    "\n",
    "df_abstracts.rename(columns={\n",
    "    'real_abstract': 'human_text',\n",
    "    'real_word_count': 'human_len',\n",
    "    'generated_abstract': 'gpt',\n",
    "    'generated_word_count': 'gpt_len'\n",
    "}, inplace=True)\n",
    "\n",
    "df_abstracts['Llama3.1'], df_abstracts['Llama3.1_len'] = None, None\n",
    "df_abstracts['Falcon'], df_abstracts['Falcon_len'] = None, None\n",
    "df_abstracts['prompt'] = None\n",
    "\n",
    "new_order = [\n",
    "    'title', 'prompt',\n",
    "    'human_text', 'human_len',\n",
    "    'gpt', 'gpt_len',\n",
    "    'Llama3.1', 'Llama3.1_len',\n",
    "    'Falcon', 'Falcon_len'\n",
    "]\n",
    "\n",
    "df_abstracts = df_abstracts[new_order]\n",
    "print(df_abstracts.columns)\n",
    "print(df_abstracts.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining methods for word/sentence count, prompt generation, model generation \n",
    "\n",
    "def count_words_and_sentences(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the text into words and sentences using nltk \n",
    "    Returns a tuple of (n_words,n_sentences)\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    return (len(words), len(sentences))\n",
    "\n",
    "def batch_count_words_and_sentences(texts):\n",
    "    \"\"\"\n",
    "    Tokenizes the text into words and sentences using nltk \n",
    "    Returns a tuple of (n_words,n_sentences)\n",
    "    \"\"\"\n",
    "    return [(len(word_tokenize(text)), len(sent_tokenize(text))) for text in texts]\n",
    "\n",
    "def create_wiki_prompt(row):\n",
    "    \"\"\"\n",
    "    Creates the wiki dataset prompt using the title and first 7 words written by humans\n",
    "    \"\"\"\n",
    "    first_few_words = ' '.join(row['human_text'].split()[:7]) \n",
    "    # prompt = (\n",
    "    #     f\"Your role is a Wikipedia contributor.\\n\"\n",
    "    #     f\"Your task is to compose a detailed Wikipedia-style introduction for the topic '{row['title']}'.\\n\"\n",
    "    #     f\"**The introduction should be exactly {row['human_len'][1]} sentences long.**\\n\"\n",
    "    #     f\"Begin with a clear definition, followed by key details and context essential for understanding the topic.\\n\"\n",
    "    #     f\"Do not include section headings, bullet points, or editing instructions.\\n\\n\"\n",
    "    #     f\"Introduction: {first_few_words}\"\n",
    "    # )\n",
    "    prompt = (\n",
    "    f\"### | Instruction | ###\\n\"\n",
    "    f\"Your role is a Wikipedia contributor.\\n\"\n",
    "    f\"Your task is to compose a detailed Wikipedia-style introduction for the topic '{row['title']}'.\\n\"\n",
    "    f\"The introduction should be at least {row['human_len'][0]} words long and {row['human_len'][1]} sentences long.\\n\"\n",
    "    f\"Begin with a clear definition, followed by key details and context essential for understanding the topic.\\n\"\n",
    "    f\"Do not include headings or editorial notes.\\n\"\n",
    "    f\"### | Response | ###\\n\"\n",
    "    f\"{first_few_words}\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def create_news_prompt(row):\n",
    "    \"\"\"\n",
    "    Creates the news dataset prompt using the first 15 words written by humans, and the article highlights\n",
    "    \"\"\"\n",
    "    first_few_words = ' '.join(row['human_text'].split()[:15]) \n",
    "    highlights = row['highlights'] \n",
    "    prompt = (\n",
    "    f\"### | Instruction | ###\\n\"\n",
    "    f\"Your role is a professional news journalist.\\n\"\n",
    "    f\"Your task is to write a detailed news article based on the given highlights.\\n\"\n",
    "    f\"The article should be at least {row['human_len'][0]} words long and {row['human_len'][1]} sentences long.\\n\"\n",
    "    f\"Incorporate the following key highlights: {highlights}\\n\"\n",
    "    f\"Begin with the most important details, followed by context and supporting information.\\n\"\n",
    "    f\"Do not include headings or editorial notes.\\n\"\n",
    "    f\"### | Response | ###\\n\"\n",
    "    f\"{first_few_words}\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def create_abstracts_prompt(row):\n",
    "    \"\"\"\n",
    "    Creates the abstracts dataset prompt using the title and first 15 words written by humans\n",
    "    \"\"\"\n",
    "    first_few_words = ' '.join(row['human_text'].split()[:15]) \n",
    "    prompt = (\n",
    "    f\"### | Instruction | ###\\n\"\n",
    "    f\"Your role is a scientist writing a research abstract for the paper titled '{row['title']}'.\\n\"\n",
    "    f\"Your task is to write a concise, detailed abstract for this paper.\\n\"\n",
    "    f\"The abstract should be at least {row['human_len'][0]} words long and {row['human_len'][1]} sentences long.\\n\"\n",
    "    f\"Begin with a clear statement of the research question or problem, followed by methodology, findings, and implications.\\n\"\n",
    "    f\"Do not include headings or editorial notes.\\n\"\n",
    "    f\"### | Response | ###\\n\"\n",
    "    f\"{first_few_words}\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def get_length_params(task_type):\n",
    "    if task_type == \"wikipedia\":\n",
    "        return {\"max_length\": 512}  \n",
    "    elif task_type == \"news\":\n",
    "        return {\"max_length\": 1024}  # For longer news articles\n",
    "    elif task_type == \"abstract\":\n",
    "        return {\"max_length\": 512} \n",
    "    else:\n",
    "        return {\"max_length\": 1024} \n",
    "\n",
    "def generate_text_gpt2xl_v2(prompt, model, tokenizer):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True)\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=1024,  # Set to the maximum length of the model\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    n_words, n_sentences = count_words_and_sentences(generated_text)\n",
    "    return generated_text, n_words, n_sentences\n",
    "\n",
    "\n",
    "def generate_text_llama2(prompt, model, tokenizer, max_length=500):\n",
    "    \"\"\"\n",
    "    Encodes the prompt using the model tokenizer\n",
    "    Returns the generated text, word count and sentence count\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to('cuda')\n",
    "        model = model.to('cuda')\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'], \n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        do_sample=True,       # Enable sampling to generate more diverse responses\n",
    "        # temperature=0.9,    # Slightly randomize the outputs to prevent repetition\n",
    "        # top_k=50,           # Consider top 50 tokens for sampling at each step\n",
    "        # top_p=0.95,          \n",
    "        top_p = 0.9,\n",
    "        num_beams = 5,\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    start_index = min((generated_text.find(key) for key in [\"Introduction:\", \"Article:\", \"Abstract:\"] if generated_text.find(key) != -1), default=0)\n",
    "    generated_text = generated_text[start_index:].strip()\n",
    "    n_words, n_sentences = count_words_and_sentences(generated_text)\n",
    "    return generated_text, n_words, n_sentences\n",
    "\n",
    "def generate_text_llama3(prompt, model, tokenizer, length_params):\n",
    "    \"\"\"\n",
    "    Encodes the prompt using the model tokenizer\n",
    "    Returns the generated text, word count and sentence count\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=512)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to('cuda')\n",
    "        model = model.to('cuda')\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'], \n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        num_return_sequences=1,\n",
    "        max_new_tokens=length_params[\"max_length\"],\n",
    "        do_sample=False,      # Enable sampling to generate more diverse responses\n",
    "        # temperature=0.5,    # Slightly randomize the outputs to prevent repetition\n",
    "        # top_k=20,           # Consider top 50 tokens for sampling at each step      \n",
    "        # top_p = 0.9,\n",
    "        num_beams = 5,     # cant run with v100 16gGB\n",
    "        no_repeat_ngram_size=5,\n",
    "        repetition_penalty=1.5,\n",
    "        length_penalty=0.9,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    match = re.search(r\"### \\| Response \\| ###\\s*(.*)\", generated_text, re.DOTALL)\n",
    "    if match:\n",
    "        response_text = match.group(1).strip()\n",
    "    else:\n",
    "        print(\"Response delimiter not found in generated text!\")\n",
    "        response_text = generated_text\n",
    "    n_words, n_sentences = count_words_and_sentences(response_text)\n",
    "    return response_text, n_words, n_sentences\n",
    "\n",
    "def generate_text_falcon(prompt, model, tokenizer, length_params):\n",
    "    \"\"\"\n",
    "    Encodes the prompt using the model tokenizer\n",
    "    Returns the generated text, word count and sentence count\n",
    "    \"\"\"\n",
    "    # Adjust tokenizer padding for decoding\n",
    "    tokenizer.padding_side = 'left'\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token or '[PAD]'\n",
    "\n",
    "    # encode the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to('cuda')\n",
    "        model = model.to('cuda')\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=length_params[\"max_length\"], \n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=4,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        do_sample=False,\n",
    "        # top_p=0.9,\n",
    "        # temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    match = re.search(r\"### \\| Response \\| ###\\s*(.*)\", generated_text, re.DOTALL)\n",
    "    if match:\n",
    "        response_text = match.group(1).strip()\n",
    "    else:\n",
    "        print(\"Response delimiter not found in generated text!\")\n",
    "        response_text = generated_text \n",
    "    n_words, n_sentences = count_words_and_sentences(response_text)\n",
    "    return response_text, n_words, n_sentences\n",
    "\n",
    "def generate_text_llama3_batch(prompts, model, tokenizer, length_params):\n",
    "    \"\"\"\n",
    "    Encodes the prompts using the model tokenizer in batches.\n",
    "    Returns the generated text, word count, and sentence count for all prompts in the batch.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "\n",
    "    # Generate output for the batch\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'], \n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        num_return_sequences=1,\n",
    "        max_new_tokens=length_params[\"max_length\"],\n",
    "        do_sample=False,\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=5,\n",
    "        repetition_penalty=1.5,\n",
    "        length_penalty=0.9,\n",
    "        early_stopping=True,\n",
    "        temperature=None, \n",
    "        top_p=None         \n",
    "    )\n",
    "    generated_texts = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "    # extract the generated sections using regex\n",
    "    response_texts = []\n",
    "    for generated_text in generated_texts:\n",
    "        match = re.search(r\"### \\| Response \\| ###\\s*(.*)\", generated_text, re.DOTALL)\n",
    "        if match:\n",
    "            response_texts.append(match.group(1).strip())\n",
    "        else:\n",
    "            print(\"Response delimiter not found in generated text!\")\n",
    "            response_texts.append(generated_text.strip())\n",
    "    lengths = batch_count_words_and_sentences(response_texts)\n",
    "    return response_texts, lengths\n",
    "\n",
    "def generate_text_falcon_batch(prompts, model, tokenizer, length_params):\n",
    "    \"\"\"\n",
    "    Encodes the prompts using the model tokenizer in batches\n",
    "    Returns the generated text, word count, and sentence count for all prompts in the batch\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=length_params[\"max_length\"],\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=4,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        do_sample=False\n",
    "    )\n",
    "    generated_texts = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    response_texts = []\n",
    "    for generated_text in generated_texts:\n",
    "        match = re.search(r\"### \\| Response \\| ###\\s*(.*)\", generated_text, re.DOTALL)\n",
    "        if match:\n",
    "            response_texts.append(match.group(1).strip())\n",
    "        else:\n",
    "            print(\"Response delimiter not found in generated text!\")\n",
    "            response_texts.append(generated_text.strip())\n",
    "\n",
    "    lengths = batch_count_words_and_sentences(response_texts)\n",
    "    return response_texts, lengths\n",
    "\n",
    "# def process_dataset_in_batches(df, model, tokenizer, batch_size, length_params, model_name, generate_method):\n",
    "#     \"\"\"\n",
    "#     Process the dataset in batches to generate text for the specified model\n",
    "#     \"\"\"\n",
    "#     for batch_df in tqdm(batch_generator(df, batch_size), total=np.ceil(len(df) / batch_size), desc=f\"Generating {model_name}\"):\n",
    "#         prompts = batch_df['prompt'].tolist()\n",
    "#         generated_texts, lengths = generate_method(prompts, model, tokenizer, length_params)\n",
    "#         df.loc[batch_df.index, model_name] = pd.Series(generated_texts, index=batch_df.index)\n",
    "#         df.loc[batch_df.index, f\"{model_name}_len\"] = pd.Series(lengths, index=batch_df.index)\n",
    "\n",
    "\n",
    "def process_dataset_in_batches(df, model, tokenizer, batch_size, length_params, model_name, generate_method, output_path, checkpoint_path, target_rows):\n",
    "    \"\"\"\n",
    "    Process the dataset in batches and save results incrementally.\n",
    "    Supports resuming from a checkpoint and ensures each model processes up to target rows considering already generated rows.\n",
    "    \"\"\"\n",
    "    # Initialize \n",
    "    processed_count = 0\n",
    "    start_index = 0\n",
    "\n",
    "    # output file\n",
    "    if not os.path.exists(output_path):\n",
    "        df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "    output_df = pd.read_csv(output_path)\n",
    "    if model_name not in output_df.columns:\n",
    "        output_df[model_name] = None  # Initialize the column if it doesn't exist\n",
    "    output_df[model_name] = output_df[model_name].astype(object)  # Explicitly set as object type\n",
    "\n",
    "    length_column = f\"{model_name}_len\"\n",
    "    if length_column not in output_df.columns:\n",
    "        output_df[length_column] = None  # Initialize the column if it doesn't exist\n",
    "    output_df[length_column] = output_df[length_column].astype(object) \n",
    "\n",
    "    # check how many rows within the first `target_rows` indices were generated\n",
    "    first_target_indices = output_df.index[:target_rows] \n",
    "    processed_rows = output_df.loc[first_target_indices, model_name].notna()\n",
    "    processed_count = processed_rows.sum()\n",
    "    print(f\"{processed_count} out of {target_rows} rows already generated for {model_name}.\")\n",
    "\n",
    "    # rows that need to be generated\n",
    "    remaining_to_target = max(0, target_rows - processed_count)\n",
    "    unprocessed_indices = output_df[output_df[model_name].isna()].index[:remaining_to_target]\n",
    "    rows_to_generate = unprocessed_indices[:remaining_to_target]\n",
    "    \n",
    "    if remaining_to_target == 0:\n",
    "        print(f\"Target of {target_rows} rows already reached for {model_name}. Skipping further processing.\")\n",
    "        return\n",
    "    rows_to_generate = rows_to_generate[:remaining_to_target]\n",
    "\n",
    "    # total batches for progress tracking\n",
    "    total_batches = (len(unprocessed_indices) + batch_size - 1) // batch_size\n",
    "\n",
    "    # generate in batches\n",
    "    with tqdm(total=total_batches, desc=f\"Generating {model_name}\", unit=\"batch\") as pbar:\n",
    "        for batch_start in range(0, len(rows_to_generate), batch_size):\n",
    "            batch_indices = rows_to_generate[batch_start:batch_start + batch_size]\n",
    "            batch_df = df.loc[batch_indices].reset_index(drop=True)\n",
    "\n",
    "            # Generate text\n",
    "            prompts = batch_df['prompt'].tolist()\n",
    "            generated_texts, lengths = generate_method(prompts, model, tokenizer, length_params)\n",
    "\n",
    "            # insert to df\n",
    "            for i, (text, length) in enumerate(zip(generated_texts, lengths)):\n",
    "                output_df.loc[batch_indices[i], model_name] = text\n",
    "                output_df.loc[batch_indices[i], length_column] = str(length)\n",
    "\n",
    "            # save\n",
    "            processed_count += len(batch_indices)\n",
    "            with open(checkpoint_path, 'w') as f_ckpt:\n",
    "                f_ckpt.write(str(processed_count))\n",
    "            output_df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "            \n",
    "            pbar.update(1)\n",
    "            print(f\"Processed batch of {len(batch_indices)} rows. Total processed: {processed_count}.\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "def batch_generator(df, batch_size, start_index=0):\n",
    "    \"\"\"\n",
    "    Splits the DataFrame into batches starting from `start_index`\n",
    "    Yields (start_index, batch_df) for each batch\n",
    "    \"\"\"\n",
    "    for i in range(start_index, len(df), batch_size):\n",
    "        yield i, df.iloc[i:i + batch_size]\n",
    "\n",
    "def clear_gpu_memory(model, tokenizer):\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate length of human text with tuple(word_count, sentence_count)\n",
    "df_wiki['human_len'] = batch_count_words_and_sentences(df_wiki['human_text'].tolist())\n",
    "df_news['human_len'] = batch_count_words_and_sentences(df_news['human_text'].tolist())\n",
    "df_abstracts['human_len'] = batch_count_words_and_sentences(df_abstracts['human_text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate length of GPT text with tuple(word_count, sentence_count)\n",
    "df_wiki['gpt_len'] = batch_count_words_and_sentences(df_wiki['gpt'].tolist())\n",
    "df_news['gpt_len'] = batch_count_words_and_sentences(df_news['gpt'].tolist())\n",
    "df_abstracts['gpt_len'] = batch_count_words_and_sentences(df_abstracts['gpt'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prompts\n",
    "df_wiki['prompt'] = df_wiki.iloc[0:2500].apply(create_wiki_prompt, axis=1)\n",
    "df_news['prompt'] = df_news.iloc[0:2500].apply(create_news_prompt, axis=1)\n",
    "df_abstracts['prompt'] = df_abstracts.iloc[0:2500].apply(create_abstracts_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save loaded datasets\n",
    "df_wiki[0:2500].to_csv('src/wiki_dataset.csv', index=False, encoding=\"utf-8-sig\")\n",
    "df_news[0:2500].to_csv('src/news_dataset.csv', index=False, encoding=\"utf-8-sig\")\n",
    "df_abstracts[0:2500].to_csv('src/abstracts_dataset.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "### Part 2 ###\n",
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia - Average sentences: 9.162501582478795\n",
      "Wikipedia - Median sentences: 9.0\n",
      "Wikipedia - Minimum sentences: 1\n",
      "Wikipedia - Maximum sentences: 71\n",
      "News - Average sentences: 22.985335892514396\n",
      "News - Median sentences: 22.0\n",
      "News - Minimum sentences: 9\n",
      "News - Maximum sentences: 126\n",
      "Abstracts - Average sentences: 8.0332\n",
      "Abstracts - Median sentences: 8.0\n",
      "Abstracts - Minimum sentences: 1\n",
      "Abstracts - Maximum sentences: 35\n"
     ]
    }
   ],
   "source": [
    "# domain articles length stats for tuning max length generation \n",
    "def calc_sentence_stats(df, task_name):\n",
    "    sentence_counts = [t[1] for t in df['human_len']]\n",
    "    avg_sentences = pd.Series(sentence_counts).mean()\n",
    "    median_sentences = pd.Series(sentence_counts).median()\n",
    "    min_sentences = pd.Series(sentence_counts).min()\n",
    "    max_sentences = pd.Series(sentence_counts).max()\n",
    "    \n",
    "    print(f\"{task_name} - Average sentences: {avg_sentences}\")\n",
    "    print(f\"{task_name} - Median sentences: {median_sentences}\")\n",
    "    print(f\"{task_name} - Minimum sentences: {min_sentences}\")\n",
    "    print(f\"{task_name} - Maximum sentences: {max_sentences}\")\n",
    "\n",
    "calc_sentence_stats(df_wiki, 'Wikipedia')\n",
    "calc_sentence_stats(df_news, 'News')\n",
    "calc_sentence_stats(df_abstracts, 'Abstracts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #### Toy example to test llama3.1 8B model and tokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "# # model = model.half() # required for 16GB gpu (8B 32float = 32gb) so reduce to 16float\n",
    "# if torch.cuda.is_available():\n",
    "#     model.cuda()  \n",
    "# model.eval()\n",
    "\n",
    "# prompt = (\n",
    "#     f\"Your role is a Wikipedia contributor. \"\n",
    "#     f\"Compose a Wikipedia-style introduction for the topic 'Moluccans'. \"\n",
    "#     f\"Start with a clear definition, followed by key details and context that is essential for understanding the subject. \"\n",
    "#     f\"Ensure the introduction is detailed and spans approximately 11 sentences long. \"\n",
    "#     f\"Introduction: Moluccans are the Austronesian-speaking and Papuan-speaking ethnic\"\n",
    "# )\n",
    "\n",
    "# inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=4096)\n",
    "# if torch.cuda.is_available():\n",
    "#     inputs = inputs.to('cuda')\n",
    "#     model = model.to('cuda')\n",
    "# output_ids = model.generate(\n",
    "#     input_ids=inputs['input_ids'], \n",
    "#     attention_mask=inputs['attention_mask'],\n",
    "#     do_sample=True,       # Enable sampling to generate more diverse responses\n",
    "#     # temperature=0.9,    # Slightly randomize the outputs to prevent repetition\n",
    "#     # top_k=50,           # Consider top 50 tokens for sampling at each step\n",
    "#     # top_p=0.95,          \n",
    "#     top_p = 0.9,\n",
    "#     # num_beams = 5,\n",
    "#     temperature=0.7,\n",
    "#     num_return_sequences=1,\n",
    "#     max_length = 512\n",
    "# )\n",
    "\n",
    "# generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "# start_index = -1\n",
    "# for keyword in [\"Introduction:\", \"Article:\", \"Abstract:\"]:\n",
    "#     start_index = generated_text.find(keyword)\n",
    "#     if start_index != -1:\n",
    "#         start_index += len(keyword)\n",
    "#         break  # Stop once we find a valid keyword\n",
    "\n",
    "# if start_index != -1:\n",
    "#     generated_text = generated_text[start_index:].strip()\n",
    "# else:\n",
    "#     print(\"Keyword not found in generated text!\")\n",
    "\n",
    "# n_words, n_sentences = count_words_and_sentences(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Toy example to test model and tokenizer\n",
    "# Load falcon 7B tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\n",
    "# if torch.cuda.is_available():\n",
    "#     model.cuda()  \n",
    "# model.eval()\n",
    "\n",
    "# tokenizer.padding_side = 'left'\n",
    "# if tokenizer.pad_token is None:\n",
    "#     if tokenizer.eos_token:\n",
    "#         tokenizer.pad_token = tokenizer.eos_token\n",
    "#     else:\n",
    "#         tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# # Encode the prompt to tensor of input ids\n",
    "# inputs = tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=4096)\n",
    "# if torch.cuda.is_available():\n",
    "#     inputs = inputs.to('cuda')\n",
    "#     model = model.to('cuda')\n",
    "\n",
    "# output_ids = model.generate(\n",
    "#     input_ids=inputs['input_ids'],\n",
    "#     attention_mask=inputs['attention_mask'],\n",
    "#     max_length=4096, \n",
    "#     num_return_sequences=1,\n",
    "#     no_repeat_ngram_size=4,  # Prevents the model from repeating the same 4-gram\n",
    "#     top_p=0.92,\n",
    "#     top_k=50,\n",
    "#     temperature=0.7\n",
    "# )\n",
    "\n",
    "# generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True) \n",
    "# n_words, n_sentences = count_words_and_sentences(generated_text)\n",
    "\n",
    "# ##### Toy example to test model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define batch size, load lama 3.1, move to cuda, ensure pad token for tokenizer\n",
    "# batch_size = 16 \n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "# tokenizer.padding_side = \"left\"\n",
    "# if tokenizer.pad_token is None:  # Set pad_token to eos_token if not set\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "# # model = model.half() # required for 16GB gpu, (8B 32 float = 32gb)\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.to('cuda')\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate text for wiki dataset\n",
    "# task_type = \"wikipedia\"\n",
    "# length_params = get_length_params(task_type)\n",
    "# process_dataset_in_batches(df_wiki[0:99], model, tokenizer, batch_size, length_params, \"Llama3.1\", generate_text_llama3_batch)\n",
    "\n",
    "# # generate text for news dataset\n",
    "# task_type = \"news\"\n",
    "# length_params = get_length_params(task_type)\n",
    "# process_dataset_in_batches(df_news[0:99], model, tokenizer, batch_size, length_params, \"Llama3.1\", generate_text_llama3_batch)\n",
    "\n",
    "# # generate text for abstracts dataset\n",
    "# task_type = \"abstract\"\n",
    "# length_params = get_length_params(task_type)\n",
    "# process_dataset_in_batches(df_abstracts[0:99], model, tokenizer, batch_size, length_params, \"Llama3.1\", generate_text_llama3_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clear LLaMA 3.1 8B model and tokenizer from GPU memory\n",
    "# del model  \n",
    "# del tokenizer \n",
    "# torch.cuda.empty_cache()  # Clear the GPU cache\n",
    "# gc.collect()              # Run Python garbage collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load falcon 7B tokenizer and model\n",
    "\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\n",
    "# # model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"left\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b\")\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.to('cuda')\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate text for wiki dataset\n",
    "# task_type = \"wikipedia\"\n",
    "# length_params = get_length_params(task_type)\n",
    "# process_dataset_in_batches(df_wiki[0:20], model, tokenizer, batch_size, length_params, \"Falcon\", generate_text_falcon_batch)\n",
    "\n",
    "# # generate text for news dataset\n",
    "# task_type = \"news\"\n",
    "# length_params = get_length_params(task_type)\n",
    "# process_dataset_in_batches(df_news[0:20], model, tokenizer, batch_size, length_params, \"Falcon\", generate_text_falcon_batch)\n",
    "\n",
    "# # generate text for abstracts dataset\n",
    "# task_type = \"abstract\"\n",
    "# length_params = get_length_params(task_type)\n",
    "# process_dataset_in_batches(df_abstracts[0:20], model, tokenizer, batch_size, length_params, \"Falcon\", generate_text_falcon_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export to CSV for log-ppx response calculation\n",
    "# # Save the datasets with UTF-8 BOM encoding for compatibility with Excel\n",
    "\n",
    "# df_wiki.iloc[0:100].to_csv('src/wiki_dataset.csv', index=False, encoding=\"utf-8-sig\")\n",
    "# df_news.iloc[0:100].to_csv('src/news_dataset.csv', index=False, encoding=\"utf-8-sig\")\n",
    "# df_abstracts.iloc[0:100].to_csv('src/abstracts_dataset.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for generation\n",
    "wiki_path = \"src/wiki_dataset.csv\"\n",
    "news_path = \"src/news_dataset.csv\"\n",
    "abstracts_path = \"src/abstracts_dataset.csv\"\n",
    "\n",
    "output_wiki = \"src/wiki_dataset_output.csv\"\n",
    "output_news = \"src/news_dataset_output.csv\"\n",
    "output_abstracts = \"src/abstracts_dataset_output.csv\"\n",
    "\n",
    "checkpoint_wiki = \"src/wiki_checkpoint.txt\"\n",
    "checkpoint_news = \"src/news_checkpoint.txt\"\n",
    "checkpoint_abstracts = \"src/abstracts_checkpoint.txt\"\n",
    "\n",
    "# Process datasets for LLaMA 3.1\n",
    "datasets = [\n",
    "    (\"wikipedia\", wiki_path, output_wiki, checkpoint_wiki),\n",
    "    (\"news\", news_path, output_news, checkpoint_news),\n",
    "    (\"abstract\", abstracts_path, output_abstracts, checkpoint_abstracts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d85a7bd02b4116bc01cf121a14fd91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Llama 3.1, move to cuda, ensure pad token for tokenizer, define batch size\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token is None:  # Set pad_token to eos_token if not set\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "# model = model.half() # required for 16GB gpu, (8B 32 float = 32gb)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "model.eval()\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing wikipedia dataset with Llama 3.1.\n",
      "120 out of 170 rows already generated for Llama3.1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:   0%|                                               | 0/7 [00:00<?, ?batch/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating Llama3.1:  14%|█████▌                                 | 1/7 [01:22<08:14, 82.38s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:  29%|███████████▏                           | 2/7 [02:45<06:53, 82.60s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 136.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:  43%|████████████████▋                      | 3/7 [04:08<05:32, 83.09s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 144.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:  57%|██████████████████████▎                | 4/7 [05:31<04:08, 82.93s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 152.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:  71%|███████████████████████████▊           | 5/7 [06:53<02:45, 82.68s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 160.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:  86%|█████████████████████████████████▍     | 6/7 [08:16<01:22, 82.87s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 168.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1: 100%|███████████████████████████████████████| 7/7 [08:44<00:00, 75.00s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 2 rows. Total processed: 170.\n",
      "Processing news dataset with Llama 3.1.\n",
      "120 out of 170 rows already generated for Llama3.1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:   0%|                                               | 0/7 [00:00<?, ?batch/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating Llama3.1:  14%|█████▍                                | 1/7 [02:05<12:34, 125.79s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:  29%|██████████▊                           | 2/7 [05:49<15:17, 183.55s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 136.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:  43%|████████████████▎                     | 3/7 [09:42<13:43, 205.81s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 144.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:  57%|█████████████████████▋                | 4/7 [12:12<09:11, 183.87s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 152.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:  71%|███████████████████████████▏          | 5/7 [14:21<05:28, 164.12s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 160.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:  86%|████████████████████████████████▌     | 6/7 [15:57<02:21, 141.01s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 168.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1: 100%|██████████████████████████████████████| 7/7 [16:43<00:00, 143.30s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 2 rows. Total processed: 170.\n",
      "Processing abstract dataset with Llama 3.1.\n",
      "120 out of 170 rows already generated for Llama3.1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:   0%|                                               | 0/7 [00:00<?, ?batch/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating Llama3.1:  14%|█████▌                                 | 1/7 [01:32<09:15, 92.54s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:  29%|███████████▏                           | 2/7 [03:03<07:38, 91.63s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 136.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:  43%|████████████████▋                      | 3/7 [04:30<05:57, 89.30s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 144.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:  57%|██████████████████████▎                | 4/7 [05:57<04:25, 88.58s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 152.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:  71%|███████████████████████████▊           | 5/7 [07:14<02:48, 84.29s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 160.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:  86%|█████████████████████████████████▍     | 6/7 [08:43<01:25, 85.93s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 168.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1: 100%|███████████████████████████████████████| 7/7 [09:05<00:00, 77.91s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 2 rows. Total processed: 170.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate Llama 3.1\n",
    "for task_type, input_path, output_path, checkpoint_path in datasets:\n",
    "    print(f\"Processing {task_type} dataset with Llama 3.1.\")\n",
    "    df = pd.read_csv(input_path)\n",
    "    length_params = get_length_params(task_type)\n",
    "    process_dataset_in_batches(\n",
    "        df, model, tokenizer, batch_size, length_params,\n",
    "        \"Llama3.1\", generate_text_llama3_batch,\n",
    "        output_path, checkpoint_path, target_rows=170\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_gpu_memory(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4289df5dbe6a4b1fa4be021111f5c334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Falcon 7B tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b\")\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "model.eval()\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing wikipedia dataset with Falcon\n",
      "170 out of 170 rows already generated for Falcon.\n",
      "Target of 170 rows already reached for Falcon. Skipping further processing.\n",
      "Processing news dataset with Falcon\n",
      "120 out of 170 rows already generated for Falcon.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Falcon:   0%|                                                 | 0/7 [00:00<?, ?batch/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating Falcon:  14%|█████▋                                  | 1/7 [02:23<14:18, 143.06s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Falcon:  29%|███████████▍                            | 2/7 [04:56<12:26, 149.22s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 136.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Falcon:  43%|█████████████████▏                      | 3/7 [06:57<09:05, 136.32s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 144.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Falcon:  57%|██████████████████████▊                 | 4/7 [09:07<06:41, 133.69s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 152.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Falcon:  71%|████████████████████████████▌           | 5/7 [11:38<04:40, 140.01s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 160.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Falcon:  86%|██████████████████████████████████▎     | 6/7 [13:23<02:08, 128.30s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 168.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Falcon: 100%|████████████████████████████████████████| 7/7 [14:04<00:00, 120.64s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 2 rows. Total processed: 170.\n",
      "Processing abstract dataset with Falcon\n",
      "120 out of 170 rows already generated for Falcon.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Falcon:   0%|                                                 | 0/7 [00:00<?, ?batch/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating Falcon:  14%|█████▊                                   | 1/7 [00:47<04:46, 47.73s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Falcon:  29%|███████████▋                             | 2/7 [01:37<04:03, 48.78s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 136.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Falcon:  43%|█████████████████▌                       | 3/7 [02:29<03:20, 50.15s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 144.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Falcon:  57%|███████████████████████▍                 | 4/7 [03:20<02:31, 50.63s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 152.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Falcon:  71%|█████████████████████████████▎           | 5/7 [04:06<01:38, 49.10s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 160.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Falcon:  86%|███████████████████████████████████▏     | 6/7 [04:56<00:49, 49.32s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 168.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Falcon: 100%|█████████████████████████████████████████| 7/7 [05:12<00:00, 44.65s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 2 rows. Total processed: 170.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate Falcon\n",
    "for task_type, input_path, output_path, checkpoint_path in datasets:\n",
    "    print(f\"Processing {task_type} dataset with Falcon\")\n",
    "    df = pd.read_csv(input_path)\n",
    "    length_params = get_length_params(task_type)\n",
    "    process_dataset_in_batches(\n",
    "        df, model, tokenizer, batch_size, length_params,\n",
    "        \"Falcon\", generate_text_falcon_batch,\n",
    "        output_path, checkpoint_path, target_rows=170\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
