{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch  \n",
    "# !pip install datasets  \n",
    "# !hostname\n",
    "# !pip install --upgrade transformers\n",
    "# !pip install spacy\n",
    "# !pip install ipywidgets\n",
    "# !pip install --upgrade transformers \"numpy<2\" \"pyarrow<14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# This notebook consists of two main parts:\n",
    "#   1. Loading of the datasets, populating article lengths, crating the prompts and saving to domain_dataset.csv\n",
    "#   2. Generating articles for each model in batches and save results incrementally to domain_dataset_out.csv\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec  7 19:17:49 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off |   00000001:00:00.0 Off |                    0 |\n",
      "| N/A   34C    P0             42W /  300W |       6MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# !nvcc --version\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/akp3user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import set_seed\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from PrepareSentenceContext import PrepareSentenceContext\n",
    "\n",
    "nltk.download('punkt')\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu124\n",
      "CUDA is available: True\n",
      "CUDA version: 12.4\n"
     ]
    }
   ],
   "source": [
    "# Check current PyTorch and CUDA availability\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # notebook_login()\n",
    "# from huggingface_hub import login\n",
    "# login(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "### Part 1 ###\n",
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'url', 'title', 'title_len', 'prompt', 'human_text', 'human_len',\n",
      "       'gpt', 'gpt_len', 'Llama3.1', 'Llama3.1_len', 'Falcon', 'Falcon_len'],\n",
      "      dtype='object')\n",
      "39495\n"
     ]
    }
   ],
   "source": [
    "# Load the HuggingFace wiki_intro_long dataset\n",
    "hf_wiki_dataset = load_dataset('alonkipnis/wiki-intro-long', split='train')\n",
    "df_wiki = pd.DataFrame(hf_wiki_dataset)\n",
    "\n",
    "# Add columns for Llama2 and Falcon7B model outputs\n",
    "df_wiki['human_len'] = None\n",
    "df_wiki['gpt_len'] = None\n",
    "df_wiki['Llama3.1'], df_wiki['Llama3.1_len'] = None, None\n",
    "df_wiki['Falcon'], df_wiki['Falcon_len'] = None, None\n",
    "\n",
    "df_wiki.rename(columns={\n",
    "    'wiki_intro': 'human_text',\n",
    "    'generated_intro': 'gpt'\n",
    "    }, inplace=True)\n",
    "\n",
    "columns_to_drop = ['prompt_tokens', 'generated_text', 'generated_intro_len']\n",
    "df_wiki.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "new_order = [\n",
    "    'id', 'url', 'title', 'title_len', 'prompt',\n",
    "    'human_text', 'human_len',\n",
    "    'gpt', 'gpt_len',\n",
    "    'Llama3.1', 'Llama3.1_len',\n",
    "    'Falcon', 'Falcon_len']\n",
    "\n",
    "df_wiki = df_wiki[new_order]\n",
    "print(df_wiki.columns)\n",
    "print(df_wiki.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'highlights', 'prompt', 'human_text', 'human_len', 'gpt',\n",
      "       'gpt_len', 'Llama3.1', 'Llama3.1_len', 'Falcon', 'Falcon_len'],\n",
      "      dtype='object')\n",
      "13025\n"
     ]
    }
   ],
   "source": [
    "# Load the HuggingFace news dataset\n",
    "hf_news_dataset = load_dataset('alonkipnis/news-chatgpt-long', split='train')\n",
    "df_news = pd.DataFrame(hf_news_dataset)\n",
    "\n",
    "df_news.rename(columns={\n",
    "    'article': 'human_text',\n",
    "    'chatgpt': 'gpt'\n",
    "}, inplace=True)\n",
    "\n",
    "df_news['human_len'], df_news['gpt_len'] = None, None\n",
    "df_news['Llama3.1'], df_news['Llama3.1_len'] = None, None\n",
    "df_news['Falcon'], df_news['Falcon_len'] = None, None\n",
    "df_news['prompt'] = None\n",
    "\n",
    "new_order = [\n",
    "    'id', 'highlights', 'prompt',\n",
    "    'human_text', 'human_len',\n",
    "    'gpt', 'gpt_len',\n",
    "    'Llama3.1', 'Llama3.1_len',\n",
    "    'Falcon', 'Falcon_len'\n",
    "]\n",
    "\n",
    "df_news = df_news[new_order]\n",
    "print(df_news.columns)\n",
    "print(df_news.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'prompt', 'human_text', 'human_len', 'gpt', 'gpt_len',\n",
      "       'Llama3.1', 'Llama3.1_len', 'Falcon', 'Falcon_len'],\n",
      "      dtype='object')\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Load the HuggingFace research absracts dataset\n",
    "hf_abstracts_dataset = load_dataset('NicolaiSivesind/ChatGPT-Research-Abstracts', split='train')\n",
    "df_abstracts = pd.DataFrame(hf_abstracts_dataset)\n",
    "\n",
    "df_abstracts.rename(columns={\n",
    "    'real_abstract': 'human_text',\n",
    "    'real_word_count': 'human_len',\n",
    "    'generated_abstract': 'gpt',\n",
    "    'generated_word_count': 'gpt_len'\n",
    "}, inplace=True)\n",
    "\n",
    "df_abstracts['Llama3.1'], df_abstracts['Llama3.1_len'] = None, None\n",
    "df_abstracts['Falcon'], df_abstracts['Falcon_len'] = None, None\n",
    "df_abstracts['prompt'] = None\n",
    "\n",
    "new_order = [\n",
    "    'title', 'prompt',\n",
    "    'human_text', 'human_len',\n",
    "    'gpt', 'gpt_len',\n",
    "    'Llama3.1', 'Llama3.1_len',\n",
    "    'Falcon', 'Falcon_len'\n",
    "]\n",
    "\n",
    "df_abstracts = df_abstracts[new_order]\n",
    "print(df_abstracts.columns)\n",
    "print(df_abstracts.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining methods for word/sentence count, prompt generation, model generation \n",
    "\n",
    "def count_words_and_sentences(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the text into words and sentences using nltk \n",
    "    Returns a tuple of (n_words,n_sentences)\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    return (len(words), len(sentences))\n",
    "\n",
    "def batch_count_words_and_sentences(texts):\n",
    "    \"\"\"\n",
    "    Tokenizes the text into words and sentences using nltk \n",
    "    Returns a tuple of (n_words,n_sentences)\n",
    "    \"\"\"\n",
    "    return [(len(word_tokenize(text)), len(sent_tokenize(text))) for text in texts]\n",
    "\n",
    "def create_wiki_prompt(row):\n",
    "    \"\"\"\n",
    "    Creates the wiki dataset prompt using the title and first 7 words written by humans\n",
    "    \"\"\"\n",
    "    first_few_words = ' '.join(row['human_text'].split()[:7]) \n",
    "    # prompt = (\n",
    "    #     f\"Your role is a Wikipedia contributor.\\n\"\n",
    "    #     f\"Your task is to compose a detailed Wikipedia-style introduction for the topic '{row['title']}'.\\n\"\n",
    "    #     f\"**The introduction should be exactly {row['human_len'][1]} sentences long.**\\n\"\n",
    "    #     f\"Begin with a clear definition, followed by key details and context essential for understanding the topic.\\n\"\n",
    "    #     f\"Do not include section headings, bullet points, or editing instructions.\\n\\n\"\n",
    "    #     f\"Introduction: {first_few_words}\"\n",
    "    # )\n",
    "    prompt = (\n",
    "    f\"### | Instruction | ###\\n\"\n",
    "    f\"Your role is a Wikipedia contributor.\\n\"\n",
    "    f\"Your task is to compose a detailed Wikipedia-style introduction for the topic '{row['title']}'.\\n\"\n",
    "    f\"The introduction should be at least {row['human_len'][0]} words long and {row['human_len'][1]} sentences long.\\n\"\n",
    "    f\"Begin with a clear definition, followed by key details and context essential for understanding the topic.\\n\"\n",
    "    f\"Do not include headings or editorial notes.\\n\"\n",
    "    f\"### | Response | ###\\n\"\n",
    "    f\"{first_few_words}\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def create_news_prompt(row):\n",
    "    \"\"\"\n",
    "    Creates the news dataset prompt using the first 15 words written by humans, and the article highlights\n",
    "    \"\"\"\n",
    "    first_few_words = ' '.join(row['human_text'].split()[:15]) \n",
    "    highlights = row['highlights'] \n",
    "    prompt = (\n",
    "    f\"### | Instruction | ###\\n\"\n",
    "    f\"Your role is a professional news journalist.\\n\"\n",
    "    f\"Your task is to write a detailed news article based on the given highlights.\\n\"\n",
    "    f\"The article should be at least {row['human_len'][0]} words long and {row['human_len'][1]} sentences long.\\n\"\n",
    "    f\"Incorporate the following key highlights: {highlights}\\n\"\n",
    "    f\"Begin with the most important details, followed by context and supporting information.\\n\"\n",
    "    f\"Do not include headings or editorial notes.\\n\"\n",
    "    f\"### | Response | ###\\n\"\n",
    "    f\"{first_few_words}\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def create_abstracts_prompt(row):\n",
    "    \"\"\"\n",
    "    Creates the abstracts dataset prompt using the title and first 15 words written by humans\n",
    "    \"\"\"\n",
    "    first_few_words = ' '.join(row['human_text'].split()[:15]) \n",
    "    prompt = (\n",
    "    f\"### | Instruction | ###\\n\"\n",
    "    f\"Your role is a scientist writing a research abstract for the paper titled '{row['title']}'.\\n\"\n",
    "    f\"Your task is to write a concise, detailed abstract for this paper.\\n\"\n",
    "    f\"The abstract should be at least {row['human_len'][0]} words long and {row['human_len'][1]} sentences long.\\n\"\n",
    "    f\"Begin with a clear statement of the research question or problem, followed by methodology, findings, and implications.\\n\"\n",
    "    f\"Do not include headings or editorial notes.\\n\"\n",
    "    f\"### | Response | ###\\n\"\n",
    "    f\"{first_few_words}\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def get_length_params(task_type):\n",
    "    if task_type == \"wikipedia\":\n",
    "        return {\"max_length\": 512}  \n",
    "    elif task_type == \"news\":\n",
    "        return {\"max_length\": 1024}  # For longer news articles\n",
    "    elif task_type == \"abstract\":\n",
    "        return {\"max_length\": 512} \n",
    "    else:\n",
    "        return {\"max_length\": 1024} \n",
    "\n",
    "def generate_text_gpt2xl_v2(prompt, model, tokenizer):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True)\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=1024,  # Set to the maximum length of the model\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    n_words, n_sentences = count_words_and_sentences(generated_text)\n",
    "    return generated_text, n_words, n_sentences\n",
    "\n",
    "\n",
    "def generate_text_llama2(prompt, model, tokenizer, max_length=500):\n",
    "    \"\"\"\n",
    "    Encodes the prompt using the model tokenizer\n",
    "    Returns the generated text, word count and sentence count\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to('cuda')\n",
    "        model = model.to('cuda')\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'], \n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        do_sample=True,       # Enable sampling to generate more diverse responses\n",
    "        # temperature=0.9,    # Slightly randomize the outputs to prevent repetition\n",
    "        # top_k=50,           # Consider top 50 tokens for sampling at each step\n",
    "        # top_p=0.95,          \n",
    "        top_p = 0.9,\n",
    "        num_beams = 5,\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    start_index = min((generated_text.find(key) for key in [\"Introduction:\", \"Article:\", \"Abstract:\"] if generated_text.find(key) != -1), default=0)\n",
    "    generated_text = generated_text[start_index:].strip()\n",
    "    n_words, n_sentences = count_words_and_sentences(generated_text)\n",
    "    return generated_text, n_words, n_sentences\n",
    "\n",
    "def generate_text_llama3(prompt, model, tokenizer, length_params):\n",
    "    \"\"\"\n",
    "    Encodes the prompt using the model tokenizer\n",
    "    Returns the generated text, word count and sentence count\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=512)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to('cuda')\n",
    "        model = model.to('cuda')\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'], \n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        num_return_sequences=1,\n",
    "        max_new_tokens=length_params[\"max_length\"],\n",
    "        do_sample=False,      # Enable sampling to generate more diverse responses\n",
    "        # temperature=0.5,    # Slightly randomize the outputs to prevent repetition\n",
    "        # top_k=20,           # Consider top 50 tokens for sampling at each step      \n",
    "        # top_p = 0.9,\n",
    "        num_beams = 5,     # cant run with v100 16gGB\n",
    "        no_repeat_ngram_size=5,\n",
    "        repetition_penalty=1.5,\n",
    "        length_penalty=0.9,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    match = re.search(r\"### \\| Response \\| ###\\s*(.*)\", generated_text, re.DOTALL)\n",
    "    if match:\n",
    "        response_text = match.group(1).strip()\n",
    "    else:\n",
    "        print(\"Response delimiter not found in generated text!\")\n",
    "        response_text = generated_text\n",
    "    n_words, n_sentences = count_words_and_sentences(response_text)\n",
    "    return response_text, n_words, n_sentences\n",
    "\n",
    "def generate_text_falcon(prompt, model, tokenizer, length_params):\n",
    "    \"\"\"\n",
    "    Encodes the prompt using the model tokenizer\n",
    "    Returns the generated text, word count and sentence count\n",
    "    \"\"\"\n",
    "    # Adjust tokenizer padding for decoding\n",
    "    tokenizer.padding_side = 'left'\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token or '[PAD]'\n",
    "\n",
    "    # encode the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to('cuda')\n",
    "        model = model.to('cuda')\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=length_params[\"max_length\"], \n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=4,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        do_sample=False,\n",
    "        # top_p=0.9,\n",
    "        # temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    match = re.search(r\"### \\| Response \\| ###\\s*(.*)\", generated_text, re.DOTALL)\n",
    "    if match:\n",
    "        response_text = match.group(1).strip()\n",
    "    else:\n",
    "        print(\"Response delimiter not found in generated text!\")\n",
    "        response_text = generated_text \n",
    "    n_words, n_sentences = count_words_and_sentences(response_text)\n",
    "    return response_text, n_words, n_sentences\n",
    "\n",
    "def generate_text_llama3_batch(prompts, model, tokenizer, length_params):\n",
    "    \"\"\"\n",
    "    Encodes the prompts using the model tokenizer in batches.\n",
    "    Returns the generated text, word count, and sentence count for all prompts in the batch.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "\n",
    "    # Generate output for the batch\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'], \n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        num_return_sequences=1,\n",
    "        max_new_tokens=length_params[\"max_length\"],\n",
    "        do_sample=False,\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=5,\n",
    "        repetition_penalty=1.5,\n",
    "        length_penalty=0.9,\n",
    "        early_stopping=True,\n",
    "        temperature=None, \n",
    "        top_p=None         \n",
    "    )\n",
    "    generated_texts = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "    # extract the generated sections using regex\n",
    "    response_texts = []\n",
    "    for generated_text in generated_texts:\n",
    "        match = re.search(r\"### \\| Response \\| ###\\s*(.*)\", generated_text, re.DOTALL)\n",
    "        if match:\n",
    "            response_texts.append(match.group(1).strip())\n",
    "        else:\n",
    "            print(\"Response delimiter not found in generated text!\")\n",
    "            response_texts.append(generated_text.strip())\n",
    "    lengths = batch_count_words_and_sentences(response_texts)\n",
    "    return response_texts, lengths\n",
    "\n",
    "def generate_text_falcon_batch(prompts, model, tokenizer, length_params):\n",
    "    \"\"\"\n",
    "    Encodes the prompts using the model tokenizer in batches\n",
    "    Returns the generated text, word count, and sentence count for all prompts in the batch\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=length_params[\"max_length\"],\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=4,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        do_sample=False\n",
    "    )\n",
    "    generated_texts = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    response_texts = []\n",
    "    for generated_text in generated_texts:\n",
    "        match = re.search(r\"### \\| Response \\| ###\\s*(.*)\", generated_text, re.DOTALL)\n",
    "        if match:\n",
    "            response_texts.append(match.group(1).strip())\n",
    "        else:\n",
    "            print(\"Response delimiter not found in generated text!\")\n",
    "            response_texts.append(generated_text.strip())\n",
    "\n",
    "    lengths = batch_count_words_and_sentences(response_texts)\n",
    "    return response_texts, lengths\n",
    "\n",
    "# def process_dataset_in_batches(df, model, tokenizer, batch_size, length_params, model_name, generate_method):\n",
    "#     \"\"\"\n",
    "#     Process the dataset in batches to generate text for the specified model\n",
    "#     \"\"\"\n",
    "#     for batch_df in tqdm(batch_generator(df, batch_size), total=np.ceil(len(df) / batch_size), desc=f\"Generating {model_name}\"):\n",
    "#         prompts = batch_df['prompt'].tolist()\n",
    "#         generated_texts, lengths = generate_method(prompts, model, tokenizer, length_params)\n",
    "#         df.loc[batch_df.index, model_name] = pd.Series(generated_texts, index=batch_df.index)\n",
    "#         df.loc[batch_df.index, f\"{model_name}_len\"] = pd.Series(lengths, index=batch_df.index)\n",
    "\n",
    "\n",
    "def process_dataset_in_batches(df, model, tokenizer, batch_size, length_params, model_name, generate_method, output_path, checkpoint_path, target_rows):\n",
    "    \"\"\"\n",
    "    Process the dataset in batches and save results incrementally.\n",
    "    Supports resuming from a checkpoint and ensures each model processes up to target rows considering already generated rows.\n",
    "    \"\"\"\n",
    "    # Initialize processed count and start index\n",
    "    processed_count = 0\n",
    "    start_index = 0\n",
    "\n",
    "    # Initialize output file with the same structure as input\n",
    "    if not os.path.exists(output_path):\n",
    "        df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    # Load output file\n",
    "    output_df = pd.read_csv(output_path)\n",
    "\n",
    "    # Ensure the model column exists and has the correct type\n",
    "    if model_name not in output_df.columns:\n",
    "        output_df[model_name] = None  # Initialize the column if it doesn't exist\n",
    "    output_df[model_name] = output_df[model_name].astype(object)  # Explicitly set as object type\n",
    "\n",
    "    length_column = f\"{model_name}_len\"\n",
    "    if length_column not in output_df.columns:\n",
    "        output_df[length_column] = None  # Initialize the column if it doesn't exist\n",
    "    output_df[length_column] = output_df[length_column].astype(object)  # Explicitly set as object type\n",
    "\n",
    "    # Count rows already generated for this model\n",
    "    processed_count = output_df[model_name].notna().sum()\n",
    "    print(f\"{processed_count} rows already generated for {model_name}.\")\n",
    "\n",
    "    # Identify rows that still need to be generated\n",
    "    rows_to_generate = output_df[output_df[model_name].isna()].index.tolist()\n",
    "    remaining_to_target = max(0, target_rows - processed_count)\n",
    "\n",
    "    if remaining_to_target == 0:\n",
    "        print(f\"Target of {target_rows} rows already reached for {model_name}. Skipping further processing.\")\n",
    "        return\n",
    "\n",
    "    # Calculate the actual rows to process up to the target\n",
    "    rows_to_generate = rows_to_generate[:remaining_to_target]\n",
    "\n",
    "    # Calculate total batches for progress tracking\n",
    "    total_batches = len(rows_to_generate) // batch_size + (1 if len(rows_to_generate) % batch_size else 0)\n",
    "\n",
    "    # Process dataset in batches\n",
    "    with tqdm(total=total_batches, desc=f\"Generating {model_name}\", unit=\"batch\") as pbar:\n",
    "        for batch_start in range(0, len(rows_to_generate), batch_size):\n",
    "            # Get the batch rows\n",
    "            batch_indices = rows_to_generate[batch_start:batch_start + batch_size]\n",
    "            batch_df = df.loc[batch_indices]\n",
    "\n",
    "            # Generate text\n",
    "            prompts = batch_df['prompt'].tolist()\n",
    "            generated_texts, lengths = generate_method(prompts, model, tokenizer, length_params)\n",
    "\n",
    "            # Update model and model length columns\n",
    "            for i, (text, length) in enumerate(zip(generated_texts, lengths)):\n",
    "                output_df.loc[batch_indices[i], model_name] = text\n",
    "                output_df.loc[batch_indices[i], length_column] = str(length)\n",
    "\n",
    "            # Save progress\n",
    "            processed_count += len(batch_indices)\n",
    "            with open(checkpoint_path, 'w') as f_ckpt:\n",
    "                f_ckpt.write(str(processed_count))\n",
    "            \n",
    "            # Save the updated output DataFrame\n",
    "            output_df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "            \n",
    "            pbar.update(1)\n",
    "            print(f\"Processed batch of {len(batch_indices)} rows. Total processed: {processed_count}.\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "def batch_generator(df, batch_size, start_index=0):\n",
    "    \"\"\"\n",
    "    Splits the DataFrame into batches starting from `start_index`\n",
    "    Yields (start_index, batch_df) for each batch\n",
    "    \"\"\"\n",
    "    for i in range(start_index, len(df), batch_size):\n",
    "        yield i, df.iloc[i:i + batch_size]\n",
    "\n",
    "def clear_gpu_memory(model, tokenizer):\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate length of human text with tuple(word_count, sentence_count)\n",
    "df_wiki['human_len'] = batch_count_words_and_sentences(df_wiki['human_text'].tolist())\n",
    "df_news['human_len'] = batch_count_words_and_sentences(df_news['human_text'].tolist())\n",
    "df_abstracts['human_len'] = batch_count_words_and_sentences(df_abstracts['human_text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate length of GPT text with tuple(word_count, sentence_count)\n",
    "df_wiki['gpt_len'] = batch_count_words_and_sentences(df_wiki['gpt'].tolist())\n",
    "df_news['gpt_len'] = batch_count_words_and_sentences(df_news['gpt'].tolist())\n",
    "df_abstracts['gpt_len'] = batch_count_words_and_sentences(df_abstracts['gpt'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prompts\n",
    "df_wiki['prompt'] = df_wiki.iloc[0:2500].apply(create_wiki_prompt, axis=1)\n",
    "df_news['prompt'] = df_news.iloc[0:2500].apply(create_news_prompt, axis=1)\n",
    "df_abstracts['prompt'] = df_abstracts.iloc[0:2500].apply(create_abstracts_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save loaded datasets\n",
    "df_wiki[0:2500].to_csv('src/wiki_dataset.csv', index=False, encoding=\"utf-8-sig\")\n",
    "df_news[0:2500].to_csv('src/news_dataset.csv', index=False, encoding=\"utf-8-sig\")\n",
    "df_abstracts[0:2500].to_csv('src/abstracts_dataset.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "### Part 2 ###\n",
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia - Average sentences: 9.162501582478795\n",
      "Wikipedia - Median sentences: 9.0\n",
      "Wikipedia - Minimum sentences: 1\n",
      "Wikipedia - Maximum sentences: 71\n",
      "News - Average sentences: 22.985335892514396\n",
      "News - Median sentences: 22.0\n",
      "News - Minimum sentences: 9\n",
      "News - Maximum sentences: 126\n",
      "Abstracts - Average sentences: 8.0332\n",
      "Abstracts - Median sentences: 8.0\n",
      "Abstracts - Minimum sentences: 1\n",
      "Abstracts - Maximum sentences: 35\n"
     ]
    }
   ],
   "source": [
    "# domain articles length stats for tuning max length generation \n",
    "def calc_sentence_stats(df, task_name):\n",
    "    sentence_counts = [t[1] for t in df['human_len']]\n",
    "    avg_sentences = pd.Series(sentence_counts).mean()\n",
    "    median_sentences = pd.Series(sentence_counts).median()\n",
    "    min_sentences = pd.Series(sentence_counts).min()\n",
    "    max_sentences = pd.Series(sentence_counts).max()\n",
    "    \n",
    "    print(f\"{task_name} - Average sentences: {avg_sentences}\")\n",
    "    print(f\"{task_name} - Median sentences: {median_sentences}\")\n",
    "    print(f\"{task_name} - Minimum sentences: {min_sentences}\")\n",
    "    print(f\"{task_name} - Maximum sentences: {max_sentences}\")\n",
    "\n",
    "calc_sentence_stats(df_wiki, 'Wikipedia')\n",
    "calc_sentence_stats(df_news, 'News')\n",
    "calc_sentence_stats(df_abstracts, 'Abstracts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #### Toy example to test llama3.1 8B model and tokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "# # model = model.half() # required for 16GB gpu (8B 32float = 32gb) so reduce to 16float\n",
    "# if torch.cuda.is_available():\n",
    "#     model.cuda()  \n",
    "# model.eval()\n",
    "\n",
    "# prompt = (\n",
    "#     f\"Your role is a Wikipedia contributor. \"\n",
    "#     f\"Compose a Wikipedia-style introduction for the topic 'Moluccans'. \"\n",
    "#     f\"Start with a clear definition, followed by key details and context that is essential for understanding the subject. \"\n",
    "#     f\"Ensure the introduction is detailed and spans approximately 11 sentences long. \"\n",
    "#     f\"Introduction: Moluccans are the Austronesian-speaking and Papuan-speaking ethnic\"\n",
    "# )\n",
    "\n",
    "# inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=4096)\n",
    "# if torch.cuda.is_available():\n",
    "#     inputs = inputs.to('cuda')\n",
    "#     model = model.to('cuda')\n",
    "# output_ids = model.generate(\n",
    "#     input_ids=inputs['input_ids'], \n",
    "#     attention_mask=inputs['attention_mask'],\n",
    "#     do_sample=True,       # Enable sampling to generate more diverse responses\n",
    "#     # temperature=0.9,    # Slightly randomize the outputs to prevent repetition\n",
    "#     # top_k=50,           # Consider top 50 tokens for sampling at each step\n",
    "#     # top_p=0.95,          \n",
    "#     top_p = 0.9,\n",
    "#     # num_beams = 5,\n",
    "#     temperature=0.7,\n",
    "#     num_return_sequences=1,\n",
    "#     max_length = 512\n",
    "# )\n",
    "\n",
    "# generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "# start_index = -1\n",
    "# for keyword in [\"Introduction:\", \"Article:\", \"Abstract:\"]:\n",
    "#     start_index = generated_text.find(keyword)\n",
    "#     if start_index != -1:\n",
    "#         start_index += len(keyword)\n",
    "#         break  # Stop once we find a valid keyword\n",
    "\n",
    "# if start_index != -1:\n",
    "#     generated_text = generated_text[start_index:].strip()\n",
    "# else:\n",
    "#     print(\"Keyword not found in generated text!\")\n",
    "\n",
    "# n_words, n_sentences = count_words_and_sentences(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Toy example to test model and tokenizer\n",
    "# Load falcon 7B tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\n",
    "# if torch.cuda.is_available():\n",
    "#     model.cuda()  \n",
    "# model.eval()\n",
    "\n",
    "# tokenizer.padding_side = 'left'\n",
    "# if tokenizer.pad_token is None:\n",
    "#     if tokenizer.eos_token:\n",
    "#         tokenizer.pad_token = tokenizer.eos_token\n",
    "#     else:\n",
    "#         tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# # Encode the prompt to tensor of input ids\n",
    "# inputs = tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=4096)\n",
    "# if torch.cuda.is_available():\n",
    "#     inputs = inputs.to('cuda')\n",
    "#     model = model.to('cuda')\n",
    "\n",
    "# output_ids = model.generate(\n",
    "#     input_ids=inputs['input_ids'],\n",
    "#     attention_mask=inputs['attention_mask'],\n",
    "#     max_length=4096, \n",
    "#     num_return_sequences=1,\n",
    "#     no_repeat_ngram_size=4,  # Prevents the model from repeating the same 4-gram\n",
    "#     top_p=0.92,\n",
    "#     top_k=50,\n",
    "#     temperature=0.7\n",
    "# )\n",
    "\n",
    "# generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True) \n",
    "# n_words, n_sentences = count_words_and_sentences(generated_text)\n",
    "\n",
    "# ##### Toy example to test model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define batch size, load lama 3.1, move to cuda, ensure pad token for tokenizer\n",
    "# batch_size = 16 \n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "# tokenizer.padding_side = \"left\"\n",
    "# if tokenizer.pad_token is None:  # Set pad_token to eos_token if not set\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "# # model = model.half() # required for 16GB gpu, (8B 32 float = 32gb)\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.to('cuda')\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate text for wiki dataset\n",
    "# task_type = \"wikipedia\"\n",
    "# length_params = get_length_params(task_type)\n",
    "# process_dataset_in_batches(df_wiki[0:99], model, tokenizer, batch_size, length_params, \"Llama3.1\", generate_text_llama3_batch)\n",
    "\n",
    "# # generate text for news dataset\n",
    "# task_type = \"news\"\n",
    "# length_params = get_length_params(task_type)\n",
    "# process_dataset_in_batches(df_news[0:99], model, tokenizer, batch_size, length_params, \"Llama3.1\", generate_text_llama3_batch)\n",
    "\n",
    "# # generate text for abstracts dataset\n",
    "# task_type = \"abstract\"\n",
    "# length_params = get_length_params(task_type)\n",
    "# process_dataset_in_batches(df_abstracts[0:99], model, tokenizer, batch_size, length_params, \"Llama3.1\", generate_text_llama3_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clear LLaMA 3.1 8B model and tokenizer from GPU memory\n",
    "# del model  \n",
    "# del tokenizer \n",
    "# torch.cuda.empty_cache()  # Clear the GPU cache\n",
    "# gc.collect()              # Run Python garbage collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load falcon 7B tokenizer and model\n",
    "\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\n",
    "# # model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"left\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b\")\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.to('cuda')\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate text for wiki dataset\n",
    "# task_type = \"wikipedia\"\n",
    "# length_params = get_length_params(task_type)\n",
    "# process_dataset_in_batches(df_wiki[0:20], model, tokenizer, batch_size, length_params, \"Falcon\", generate_text_falcon_batch)\n",
    "\n",
    "# # generate text for news dataset\n",
    "# task_type = \"news\"\n",
    "# length_params = get_length_params(task_type)\n",
    "# process_dataset_in_batches(df_news[0:20], model, tokenizer, batch_size, length_params, \"Falcon\", generate_text_falcon_batch)\n",
    "\n",
    "# # generate text for abstracts dataset\n",
    "# task_type = \"abstract\"\n",
    "# length_params = get_length_params(task_type)\n",
    "# process_dataset_in_batches(df_abstracts[0:20], model, tokenizer, batch_size, length_params, \"Falcon\", generate_text_falcon_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export to CSV for log-ppx response calculation\n",
    "# # Save the datasets with UTF-8 BOM encoding for compatibility with Excel\n",
    "\n",
    "# df_wiki.iloc[0:100].to_csv('src/wiki_dataset.csv', index=False, encoding=\"utf-8-sig\")\n",
    "# df_news.iloc[0:100].to_csv('src/news_dataset.csv', index=False, encoding=\"utf-8-sig\")\n",
    "# df_abstracts.iloc[0:100].to_csv('src/abstracts_dataset.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for generation\n",
    "wiki_path = \"src/wiki_dataset.csv\"\n",
    "news_path = \"src/news_dataset.csv\"\n",
    "abstracts_path = \"src/abstracts_dataset.csv\"\n",
    "\n",
    "output_wiki = \"src/wiki_dataset_output.csv\"\n",
    "output_news = \"src/news_dataset_output.csv\"\n",
    "output_abstracts = \"src/abstracts_dataset_output.csv\"\n",
    "\n",
    "checkpoint_wiki = \"src/wiki_checkpoint.txt\"\n",
    "checkpoint_news = \"src/news_checkpoint.txt\"\n",
    "checkpoint_abstracts = \"src/abstracts_checkpoint.txt\"\n",
    "\n",
    "# Process datasets for LLaMA 3.1\n",
    "datasets = [\n",
    "    (\"wikipedia\", wiki_path, output_wiki, checkpoint_wiki),\n",
    "    (\"news\", news_path, output_news, checkpoint_news),\n",
    "    (\"abstract\", abstracts_path, output_abstracts, checkpoint_abstracts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad9761d8b1e4e25a2630c3ba1790146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Llama 3.1, move to cuda, ensure pad token for tokenizer, define batch size\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token is None:  # Set pad_token to eos_token if not set\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "# model = model.half() # required for 16GB gpu, (8B 32 float = 32gb)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "model.eval()\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing wikipedia dataset with Llama 3.1.\n",
      "112 rows already processed for Llama3.1 in src/wiki_dataset_output.csv.\n",
      "Target of 100 rows already reached for Llama3.1. Skipping.\n",
      "Processing news dataset with Llama 3.1.\n",
      "16 rows already processed for Llama3.1 in src/news_dataset_output.csv.\n",
      "Resuming from index 16 for Llama3.1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:   0%|                                                                                                                                                                                            | 0/10 [00:00<?, ?batch/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating Llama3.1:  10%|█████████████████▉                                                                                                                                                                 | 1/10 [02:10<19:34, 130.51s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 24.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:  20%|███████████████████████████████████▊                                                                                                                                               | 2/10 [05:58<25:03, 187.96s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 32.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:  30%|█████████████████████████████████████████████████████▋                                                                                                                             | 3/10 [09:46<24:02, 206.01s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 40.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Llama3.1:  30%|█████████████████████████████████████████████████████▋                                                                                                                             | 3/10 [10:48<25:13, 216.24s/batch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(input_path)\n\u001b[1;32m      5\u001b[0m length_params \u001b[38;5;241m=\u001b[39m get_length_params(task_type)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mprocess_dataset_in_batches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLlama3.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerate_text_llama3_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 334\u001b[0m, in \u001b[0;36mprocess_dataset_in_batches\u001b[0;34m(df, model, tokenizer, batch_size, length_params, model_name, generate_method, output_path, checkpoint_path, target_rows)\u001b[0m\n\u001b[1;32m    332\u001b[0m prompts \u001b[38;5;241m=\u001b[39m batch_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m     generated_texts, lengths \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRuntimeError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Skipping batch...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[37], line 214\u001b[0m, in \u001b[0;36mgenerate_text_llama3_batch\u001b[0;34m(prompts, model, tokenizer, length_params)\u001b[0m\n\u001b[1;32m    211\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# Generate output for the batch\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m         \u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m generated_texts \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(output_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# extract the generated sections using regex\u001b[39;00m\n",
      "File \u001b[0;32m~/NLP-Authorship-Attribution/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NLP-Authorship-Attribution/venv/lib/python3.12/site-packages/transformers/generation/utils.py:2246\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2239\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2240\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2241\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2242\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2243\u001b[0m     )\n\u001b[1;32m   2245\u001b[0m     \u001b[38;5;66;03m# 13. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2246\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2252\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2254\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2257\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2258\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2259\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2260\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2267\u001b[0m     )\n",
      "File \u001b[0;32m~/NLP-Authorship-Attribution/venv/lib/python3.12/site-packages/transformers/generation/utils.py:3476\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3471\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m next_token_logits\u001b[38;5;241m.\u001b[39mto(input_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3472\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlog_softmax(\n\u001b[1;32m   3473\u001b[0m     next_token_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3474\u001b[0m )  \u001b[38;5;66;03m# (batch_size * num_beams, vocab_size)\u001b[39;00m\n\u001b[0;32m-> 3476\u001b[0m next_token_scores_processed \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3477\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m next_token_scores_processed \u001b[38;5;241m+\u001b[39m beam_scores[:, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mexpand_as(\n\u001b[1;32m   3478\u001b[0m     next_token_scores_processed\n\u001b[1;32m   3479\u001b[0m )\n\u001b[1;32m   3481\u001b[0m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n",
      "File \u001b[0;32m~/NLP-Authorship-Attribution/venv/lib/python3.12/site-packages/transformers/generation/logits_process.py:104\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/NLP-Authorship-Attribution/venv/lib/python3.12/site-packages/transformers/generation/logits_process.py:974\u001b[0m, in \u001b[0;36mNoRepeatNGramLogitsProcessor.__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    972\u001b[0m cur_len \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    973\u001b[0m scores_processed \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m--> 974\u001b[0m banned_batch_tokens \u001b[38;5;241m=\u001b[39m \u001b[43m_calc_banned_ngram_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mngram_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batch_hypotheses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, banned_tokens \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(banned_batch_tokens):\n\u001b[1;32m    976\u001b[0m     scores_processed[i, banned_tokens] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/NLP-Authorship-Attribution/venv/lib/python3.12/site-packages/transformers/generation/logits_process.py:914\u001b[0m, in \u001b[0;36m_calc_banned_ngram_tokens\u001b[0;34m(ngram_size, prev_input_ids, num_hypos, cur_len)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m ngram_size:\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;66;03m# return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\u001b[39;00m\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_hypos)]\n\u001b[0;32m--> 914\u001b[0m generated_ngrams \u001b[38;5;241m=\u001b[39m \u001b[43m_get_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngram_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_hypos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    915\u001b[0m banned_tokens \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    916\u001b[0m     _get_generated_ngrams(generated_ngrams[hypo_idx], prev_input_ids[hypo_idx], ngram_size, cur_len)\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hypo_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_hypos)\n\u001b[1;32m    918\u001b[0m ]\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m banned_tokens\n",
      "File \u001b[0;32m~/NLP-Authorship-Attribution/venv/lib/python3.12/site-packages/transformers/generation/logits_process.py:875\u001b[0m, in \u001b[0;36m_get_ngrams\u001b[0;34m(ngram_size, prev_input_ids, num_hypos)\u001b[0m\n\u001b[1;32m    873\u001b[0m generated_ngrams \u001b[38;5;241m=\u001b[39m [{} \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_hypos)]\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_hypos):\n\u001b[0;32m--> 875\u001b[0m     gen_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mprev_input_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m     generated_ngram \u001b[38;5;241m=\u001b[39m generated_ngrams[idx]\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;66;03m# Loop through each n-gram of size ngram_size in the list of tokens (gen_tokens)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate Llama 3.1\n",
    "for task_type, input_path, output_path, checkpoint_path in datasets:\n",
    "    print(f\"Processing {task_type} dataset with Llama 3.1.\")\n",
    "    df = pd.read_csv(input_path)\n",
    "    length_params = get_length_params(task_type)\n",
    "    process_dataset_in_batches(\n",
    "        df, model, tokenizer, batch_size, length_params,\n",
    "        \"Llama3.1\", generate_text_llama3_batch,\n",
    "        output_path, checkpoint_path, target_rows=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_gpu_memory(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c414e28fa6814e25a17e1e9d5a6d3e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Falcon 7B tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b\")\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "model.eval()\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing wikipedia dataset with Falcon\n",
      "80 rows already generated for Falcon.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Falcon:   0%|                                                                                                                                                                                               | 0/3 [00:00<?, ?batch/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating Falcon:  33%|█████████████████████████████████████████████████████████████                                                                                                                          | 1/3 [00:55<01:50, 55.07s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 88.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Falcon:  67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                             | 2/3 [01:50<00:55, 55.08s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 96.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Falcon: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [02:07<00:00, 37.87s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 4 rows. Total processed: 100.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Falcon: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [02:07<00:00, 42.59s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing news dataset with Falcon\n",
      "0 rows already generated for Falcon.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Falcon:   0%|                                                                                                                                                                                              | 0/13 [00:00<?, ?batch/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating Falcon:   8%|█████████████▉                                                                                                                                                                       | 1/13 [02:32<30:34, 152.90s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Falcon:  15%|███████████████████████████▊                                                                                                                                                         | 2/13 [04:38<25:06, 136.92s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of 8 rows. Total processed: 16.\n"
     ]
    }
   ],
   "source": [
    "# Generate Falcon\n",
    "for task_type, input_path, output_path, checkpoint_path in datasets:\n",
    "    print(f\"Processing {task_type} dataset with Falcon\")\n",
    "    df = pd.read_csv(input_path)\n",
    "    length_params = get_length_params(task_type)\n",
    "    process_dataset_in_batches(\n",
    "        df, model, tokenizer, batch_size, length_params,\n",
    "        \"Falcon\", generate_text_falcon_batch,\n",
    "        output_path, checkpoint_path, target_rows=100\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
