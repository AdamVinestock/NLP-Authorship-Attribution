{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch  \n",
    "# !pip install datasets  \n",
    "# !hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adamvinestock/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import set_seed\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from PrepareSentenceContext import PrepareSentenceContext\n",
    "\n",
    "nltk.download('punkt')\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0\n",
      "CUDA is available: False\n",
      "CUDA version: None\n"
     ]
    }
   ],
   "source": [
    "# Check current PyTorch and CUDA availability\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aacada2e1a3f4e1caf5458bdf7e1d595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HuggingFace wiki_intro_long dataset\n",
    "hf_wiki_dataset = load_dataset('alonkipnis/wiki-intro-long', split='train')\n",
    "df_wiki = pd.DataFrame(hf_wiki_dataset)\n",
    "\n",
    "# Add columns for Llama2 and Falcon7B model outputs\n",
    "df_wiki['human_len'] = None\n",
    "df_wiki['gpt_len'] = None\n",
    "df_wiki['Llama2'], df_wiki['Llama2_len'] = None, None\n",
    "df_wiki['Falcon'], df_wiki['Falcon_len'] = None, None\n",
    "\n",
    "df_wiki.rename(columns={\n",
    "    'wiki_intro': 'human_text',\n",
    "    # 'wiki_intro_len': 'human_len',\n",
    "    'generated_intro': 'gpt'\n",
    "    }, inplace=True)\n",
    "\n",
    "columns_to_drop = ['prompt_tokens', 'generated_text', 'generated_intro_len']\n",
    "df_wiki.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "new_order = [\n",
    "    'id', 'url', 'title', 'title_len', 'prompt',\n",
    "    'human_text', 'human_len',\n",
    "    'gpt', 'gpt_len',\n",
    "    'Llama2', 'Llama2_len',\n",
    "    'Falcon', 'Falcon_len']\n",
    "\n",
    "df_wiki = df_wiki[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'url', 'title', 'title_len', 'prompt', 'human_text', 'human_len',\n",
      "       'gpt', 'gpt_len', 'Llama2', 'Llama2_len', 'Falcon', 'Falcon_len'],\n",
      "      dtype='object')\n",
      "39495\n"
     ]
    }
   ],
   "source": [
    "print(df_wiki.columns)\n",
    "print(df_wiki.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HuggingFace news dataset\n",
    "hf_news_dataset = load_dataset('alonkipnis/news-chatgpt-long', split='train')\n",
    "df_news = pd.DataFrame(hf_news_dataset)\n",
    "\n",
    "df_news.rename(columns={\n",
    "    'article': 'human_text',\n",
    "    'chatgpt': 'gpt'\n",
    "}, inplace=True)\n",
    "\n",
    "df_news['human_len'], df_news['gpt_len'] = None, None\n",
    "df_news['Llama2'], df_news['Llama2_len'] = None, None\n",
    "df_news['Falcon'], df_news['Falcon_len'] = None, None\n",
    "df_news['prompt'] = None\n",
    "\n",
    "new_order = [\n",
    "    'id', 'highlights', 'prompt',\n",
    "    'human_text', 'human_len',\n",
    "    'gpt', 'gpt_len',\n",
    "    'Llama2', 'Llama2_len',\n",
    "    'Falcon', 'Falcon_len'\n",
    "]\n",
    "\n",
    "df_news = df_news[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'highlights', 'prompt', 'human_text', 'human_len', 'gpt',\n",
      "       'gpt_len', 'Llama2', 'Llama2_len', 'Falcon', 'Falcon_len'],\n",
      "      dtype='object')\n",
      "13025\n"
     ]
    }
   ],
   "source": [
    "print(df_news.columns)\n",
    "print(df_news.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HuggingFace research absracts dataset\n",
    "hf_abstracts_dataset = load_dataset('NicolaiSivesind/ChatGPT-Research-Abstracts', split='train')\n",
    "df_abstracts = pd.DataFrame(hf_abstracts_dataset)\n",
    "\n",
    "df_abstracts.rename(columns={\n",
    "    'real_abstract': 'human_text',\n",
    "    'real_word_count': 'human_len',\n",
    "    'generated_abstract': 'gpt',\n",
    "    'generated_word_count': 'gpt_len'\n",
    "}, inplace=True)\n",
    "\n",
    "df_abstracts['Llama2'], df_abstracts['Llama2_len'] = None, None\n",
    "df_abstracts['Falcon'], df_abstracts['Falcon_len'] = None, None\n",
    "df_abstracts['prompt'] = None\n",
    "\n",
    "new_order = [\n",
    "    'title', 'prompt',\n",
    "    'human_text', 'human_len',\n",
    "    'gpt', 'gpt_len',\n",
    "    'Llama2', 'Llama2_len',\n",
    "    'Falcon', 'Falcon_len'\n",
    "]\n",
    "\n",
    "df_abstracts = df_abstracts[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'prompt', 'human_text', 'human_len', 'gpt', 'gpt_len',\n",
      "       'Llama2', 'Llama2_len', 'Falcon', 'Falcon_len'],\n",
      "      dtype='object')\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(df_abstracts.columns)\n",
    "print(df_abstracts.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_and_sentences(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the text into words and sentences using nltk \n",
    "    Returns a tuple of (n_words,n_sentences)\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    return (len(words), len(sentences))\n",
    "\n",
    "def create_wiki_prompt(row):\n",
    "    \"\"\"\n",
    "    Creates the wiki dataset prompt using the title and first 7 words written by humans\n",
    "    \"\"\"\n",
    "    first_few_words = ' '.join(row['human_text'].split()[:7]) \n",
    "    # prompt = f\"Write a Wikipedia-style intro covering the topic '{row['title']}', it should be detailed and span approximately {row['human_len'][1]} sentences long. {first_few_words}\"\n",
    "    prompt = (\n",
    "        f\"Your role is a Wikipedia contributor. \"\n",
    "        f\"Compose a Wikipedia-style introduction for the topic '{row['title']}' that spans {row['human_len'][1]} sentences long. \"\n",
    "        f\"Start with a clear definition, followed by context and key details that are essential for understanding the topic. \"\n",
    "        f\"Introduction: {first_few_words}\"\n",
    "    )\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def create_news_prompt(row):\n",
    "    \"\"\"\n",
    "    Creates the news dataset prompt using the first 15 words written by humans, and the article highlights\n",
    "    \"\"\"\n",
    "    first_few_words = ' '.join(row['human_text'].split()[:15]) \n",
    "    highlights = row['highlights'] \n",
    "    # prompt = f\"Complete the news article, make sure to be detailed, the article should span approximately {row['human_len'][1]} sentences long.\\nArticle highlights: {highlights}\\nArticle:{first_few_words}\"\n",
    "    prompt = (\n",
    "        f\"Your role is a news journalist. \"\n",
    "        f\"Write a news article based on the given highlights, ensure the article is detailed and spans approximately {row['human_len'][1]} sentences long. \"\n",
    "        f\"Incorporate the following key highlights: {highlights} \"\n",
    "        f\"Article: {first_few_words}\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def create_abstracts_prompt(row):\n",
    "    \"\"\"\n",
    "    Creates the abstracts dataset prompt using the title and first 15 words written by humans\n",
    "    \"\"\"\n",
    "    first_few_words = ' '.join(row['human_text'].split()[:15]) \n",
    "    # prompt = f\"Write a research abstract on the paper '{row['title']}'. Make sure to be detailed and span approximately {row['human_len'][1]} sentences long.\\n{first_few_words}\"\n",
    "    prompt = (\n",
    "        f\"Your role is a scientist writing a paper for publication. \"\n",
    "        f\"Write a concise research abstract for the paper titled '{row['title']}'. \"\n",
    "        f\"Ensure the abstract is detailed, clear, and spans {row['human_len'][1]} sentences long. \"\n",
    "        f\"Abstract: {first_few_words}\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def generate_text_gpt2xl(prompt, model, tokenizer, max_length=1024):\n",
    "    \"\"\"\n",
    "    Encodes the prompt using the model tokenizer - max context windows of 1024, left padding\n",
    "    Returns the generated text, word count and sentence count\n",
    "    \"\"\"\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length) # Prepare input encoding with padding and truncation\n",
    "    attention_mask = torch.fliplr(inputs['attention_mask'])                                                       # Adjust attention mask for left padding \n",
    "    max_new_tokens = 1024 - inputs['input_ids'].shape[1]                                                          # gpt2 is limited to generating 1024 token including prompt\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'], \n",
    "        attention_mask=attention_mask,  \n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    n_words, n_sentences = count_words_and_sentences(generated_text)\n",
    "    return generated_text, n_words, n_sentences\n",
    "\n",
    "def generate_text_gpt2xl_v2(prompt, model, tokenizer):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True)\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=1024,  # Set to the maximum length of the model\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    n_words, n_sentences = count_words_and_sentences(generated_text)\n",
    "    return generated_text, n_words, n_sentences\n",
    "\n",
    "\n",
    "\n",
    "def generate_text_llama2(prompt, model, tokenizer, max_length=500):\n",
    "    \"\"\"\n",
    "    Encodes the prompt using the model tokenizer\n",
    "    Returns the generated text, word count and sentence count\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to('cuda')\n",
    "        model = model.to('cuda')\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'], \n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        do_sample=True,       # Enable sampling to generate more diverse responses\n",
    "        # temperature=0.9,    # Slightly randomize the outputs to prevent repetition\n",
    "        # top_k=50,           # Consider top 50 tokens for sampling at each step\n",
    "        # top_p=0.95,          \n",
    "        top_p = 0.9,\n",
    "        num_beams = 5,\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    generated_text = generated_text[len(prompt):].strip()\n",
    "    n_words, n_sentences = count_words_and_sentences(generated_text)\n",
    "    return generated_text, n_words, n_sentences\n",
    "\n",
    "\n",
    "def generate_text_falcon(prompt, model, tokenizer, length_params):\n",
    "    \"\"\"\n",
    "    Encodes the prompt using the model tokenizer\n",
    "    Returns the generated text, word count and sentence count\n",
    "    \"\"\"\n",
    "    \n",
    "    # Adjust tokenizer padding for decoding\n",
    "    tokenizer.padding_side = 'left'\n",
    "    if tokenizer.pad_token is None:\n",
    "        if tokenizer.eos_token:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        else:\n",
    "            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    # Encode the prompt to tensor of input ids\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to('cuda')\n",
    "        model = model.to('cuda')\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=length_params[\"max_length\"], \n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=4,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        # do_sample=True,\n",
    "        # top_p=0.9,\n",
    "        # temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True) # Decode the output ids to text\n",
    "    generated_text = generated_text[len(prompt):].strip()\n",
    "    n_words, n_sentences = count_words_and_sentences(generated_text)\n",
    "    return generated_text, n_words, n_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate length of human text with tuple(word_count, sentence_count)\n",
    "df_wiki['human_len'] = df_wiki['human_text'].apply(count_words_and_sentences)\n",
    "df_news['human_len'] = df_news['human_text'].apply(count_words_and_sentences)\n",
    "df_abstracts['human_len'] = df_abstracts['human_text'].apply(count_words_and_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prompts\n",
    "df_wiki['prompt'] = df_wiki.iloc[0:10].apply(create_wiki_prompt, axis=1)\n",
    "df_news['prompt'] = df_news.iloc[0:10].apply(create_news_prompt, axis=1)\n",
    "df_abstracts['prompt'] = df_abstracts.iloc[0:10].apply(create_abstracts_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia - Average sentences: 9.162501582478795\n",
      "Wikipedia - Median sentences: 9.0\n",
      "Wikipedia - Minimum sentences: 1\n",
      "Wikipedia - Maximum sentences: 71\n",
      "News - Average sentences: 22.985335892514396\n",
      "News - Median sentences: 22.0\n",
      "News - Minimum sentences: 9\n",
      "News - Maximum sentences: 126\n",
      "Abstracts - Average sentences: 8.0332\n",
      "Abstracts - Median sentences: 8.0\n",
      "Abstracts - Minimum sentences: 1\n",
      "Abstracts - Maximum sentences: 35\n"
     ]
    }
   ],
   "source": [
    "# domain articles length stats for tuning max length generation \n",
    "def calc_sentence_stats(df, task_name):\n",
    "    sentence_counts = [t[1] for t in df['human_len']]\n",
    "    avg_sentences = pd.Series(sentence_counts).mean()\n",
    "    median_sentences = pd.Series(sentence_counts).median()\n",
    "    min_sentences = pd.Series(sentence_counts).min()\n",
    "    max_sentences = pd.Series(sentence_counts).max()\n",
    "    \n",
    "    print(f\"{task_name} - Average sentences: {avg_sentences}\")\n",
    "    print(f\"{task_name} - Median sentences: {median_sentences}\")\n",
    "    print(f\"{task_name} - Minimum sentences: {min_sentences}\")\n",
    "    print(f\"{task_name} - Maximum sentences: {max_sentences}\")\n",
    "\n",
    "calc_sentence_stats(df_wiki, 'Wikipedia')\n",
    "calc_sentence_stats(df_news, 'News')\n",
    "calc_sentence_stats(df_abstracts, 'Abstracts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length_params(task_type):\n",
    "    if task_type == \"wikipedia\":\n",
    "        return {\"max_length\": 512}  \n",
    "    elif task_type == \"news\":\n",
    "        return {\"max_length\": 728}  # For longer news articles\n",
    "    elif task_type == \"abstract\":\n",
    "        return {\"max_length\": 512} \n",
    "    else:\n",
    "        return {\"max_length\": 1024} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load gpt2-xl model and tokenizer\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
    "# tokenizer.padding_side = 'left'  # Ensure padding from the left for gpt2\n",
    "# model = GPT2LMHeadModel.from_pretrained('gpt2-xl')\n",
    "# model.eval()\n",
    "\n",
    "# for index, row in df_wiki.head(1).iterrows():  \n",
    "#     prompt = row['prompt'] \n",
    "#     generated_text, word_count, sent_count = generate_text_gpt2xl_v2(prompt, model, tokenizer)\n",
    "#     df_wiki.at[index, 'gpt'] = generated_text\n",
    "#     df_wiki.at[index, 'gpt_len'] = [(word_count, sent_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy example to test model and tokenizer\n",
    "# Load llama2 7B tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()  \n",
    "model.eval()\n",
    "\n",
    "prompt = (\n",
    "    f\"Your role is a Wikipedia contributor. \"\n",
    "    f\"Compose a Wikipedia-style introduction for the topic ''Moluccans''. \"\n",
    "    f\"Start with a clear definition, followed by key details and context that is essential for understanding the subject. \"\n",
    "    f\"Ensure the introduction is detailed and spans approximately 11 sentences long. \"\n",
    "    f\"Introduction: Moluccans are the Austronesian-speaking and Papuan-speaking ethnic\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=4096)\n",
    "if torch.cuda.is_available():\n",
    "    inputs = inputs.to('cuda')\n",
    "    model = model.to('cuda')\n",
    "output_ids = model.generate(\n",
    "    input_ids=inputs['input_ids'], \n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    do_sample=True,       # Enable sampling to generate more diverse responses\n",
    "    # temperature=0.9,    # Slightly randomize the outputs to prevent repetition\n",
    "    # top_k=50,           # Consider top 50 tokens for sampling at each step\n",
    "    # top_p=0.95,          \n",
    "    top_p = 0.9,\n",
    "    num_beams = 5,\n",
    "    temperature=0.7,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "n_words, n_sentences = count_words_and_sentences(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy example to test model and tokenizer\n",
    "# Load falcon 7B tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()  \n",
    "model.eval()\n",
    "\n",
    "\n",
    "tokenizer.padding_side = 'left'\n",
    "if tokenizer.pad_token is None:\n",
    "    if tokenizer.eos_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    else:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Encode the prompt to tensor of input ids\n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=4096)\n",
    "if torch.cuda.is_available():\n",
    "    inputs = inputs.to('cuda')\n",
    "    model = model.to('cuda')\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_length=4096, \n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=4,  # Prevents the model from repeating the same 4-gram\n",
    "    top_p=0.92,\n",
    "    top_k=50,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True) \n",
    "n_words, n_sentences = count_words_and_sentences(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load llama2 7B tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()  \n",
    "model.eval()\n",
    "\n",
    "# Generate text for the wiki dataset\n",
    "for index, row in tqdm(df_wiki.head(3).iterrows(), total=3, desc=\"Generating Wiki Dataset\"):\n",
    "    prompt = row['prompt']\n",
    "    generated_text, word_count, sent_count = generate_text_llama2(prompt, model, tokenizer)\n",
    "    df_wiki.at[index, 'Llama2'] = generated_text\n",
    "    df_wiki.at[index, 'Llama2_len'] = [(word_count, sent_count)]\n",
    "\n",
    "# Generate text for the news dataset\n",
    "for index, row in tqdm(df_news.head(3).iterrows(), total=3, desc=\"Generating News Dataset\"):\n",
    "    prompt = row['prompt']\n",
    "    generated_text, word_count, sent_count = generate_text_llama2(prompt, model, tokenizer)\n",
    "    df_news.at[index, 'Llama2'] = generated_text\n",
    "    df_news.at[index, 'Llama2_len'] = [(word_count, sent_count)]\n",
    "\n",
    "# Generate text for abstracts dataset\n",
    "for index, row in tqdm(df_abstracts.head(3).iterrows(), total=3, desc=\"Generating Abstracts Dataset\"):\n",
    "    prompt = row['prompt']\n",
    "    generated_text, word_count, sent_count = generate_text_llama2(prompt, model, tokenizer)\n",
    "    df_abstracts.at[index, 'Llama2'] = generated_text\n",
    "    df_abstracts.at[index, 'Llama2_len'] = [(word_count, sent_count)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load falcon 7B tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()  \n",
    "model.eval()\n",
    "\n",
    "# generate text for wiki dataset\n",
    "task_type = \"wikipedia\"  \n",
    "length_params = get_length_params(task_type)\n",
    "for index, row in tqdm(df_wiki.head(3).iterrows(), total=3, desc=\"Generating Wiki Dataset\"):\n",
    "    prompt = row['prompt']\n",
    "    generated_text, word_count, sent_count = generate_text_falcon(prompt, model, tokenizer)\n",
    "    df_wiki.at[index, 'Falcon'] = generated_text\n",
    "    df_wiki.at[index, 'Falcon_len'] = [(word_count, sent_count)]\n",
    "\n",
    "# generate text for news dataset\n",
    "task_type = \"news\"  \n",
    "length_params = get_length_params(task_type)\n",
    "for index, row in tqdm(df_news.head(3).iterrows(), total=3, desc=\"Generating News Dataset\"):\n",
    "    prompt = row['prompt']\n",
    "    generated_text, word_count, sent_count = generate_text_falcon(prompt, model, tokenizer)\n",
    "    df_news.at[index, 'Falcon'] = generated_text\n",
    "    df_news.at[index, 'Falcon_len'] = [(word_count, sent_count)]\n",
    "\n",
    "# generate text for abstracts dataset\n",
    "task_type = \"abstract\"  \n",
    "length_params = get_length_params(task_type)\n",
    "for index, row in tqdm(df_abstracts.head(3).iterrows(), total=3, desc=\"Generating Abstracts Dataset\"):\n",
    "    prompt = row['prompt']\n",
    "    generated_text, word_count, sent_count = generate_text_falcon(prompt, model, tokenizer)\n",
    "    df_abstracts.at[index, 'Falcon'] = generated_text\n",
    "    df_abstracts.at[index, 'Falcon_len'] = [(word_count, sent_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating context for human_text:   1%|          | 201/39495 [00:06<21:54, 29.90it/s]WARNING:root:Closing tag without opening in sentence 1: She earned the nickname Mairgréag an Einigh ('Margaret of the Hospitality') after hosting two incredible feasts in the year 1433  and went on pilgrimage to Santiago de Compostela in 1445.<ref name=FitzPatrick>{{cite journal|last1=FitzPatrick|first1=Liz|title='Mairgréag an-Einigh Ó Cearbhaill 'The best of the Women of the Gaedhil|journal=Journal of the Kildare Archaeological and Historical Society|date=1992|volume=18|pages=20–38}}</ref>.\n",
      "Creating context for human_text:   6%|▌         | 2183/39495 [01:07<19:44, 31.50it/s]WARNING:root:Closing tag without opening in sentence 9: Accessed 2012-07-06.</ref>.\n",
      "Creating context for human_text:   8%|▊         | 3295/39495 [01:42<19:07, 31.56it/s]WARNING:root:Closing tag without opening in sentence 12: name=\"ToddKillings\">{{cite web |url=https://hozacrecords.com/pre-order-sonny-vincent-1969-76-archival-lp/ }</ref>\n",
      "Creating context for human_text:  10%|▉         | 3926/39495 [02:01<17:26, 33.99it/s]WARNING:root:Closing tag without opening in sentence 7: Enoyogiere Edokpolo – The eagle on the iroko, </ref>\n",
      "Creating context for human_text:  14%|█▍        | 5650/39495 [02:55<16:09, 34.90it/s]WARNING:root:Closing tag without opening in sentence 7: It is likely that Agrippa's interest in the occult came from this Albertist influence.<ref name=\"twet55\">Goodrick-Clarke, Nicholas, The Western Esoteric Traditions', 2008, , p. 55</ref>.\n",
      "Creating context for human_text:  15%|█▌        | 6071/39495 [03:08<17:20, 32.14it/s]WARNING:root:Closing tag without opening in sentence 2: |url=https://powo.science.kew.org/taxon/urn:lsid:ipni.org:names:84179-1 |website=Plants of the World Online |publisher=Royal Botanic Gardens Kew |access-date=4 February 2022}}</ref>.\n",
      "Creating context for human_text:  16%|█▋        | 6440/39495 [03:19<16:41, 33.01it/s]WARNING:root:Closing tag without opening in sentence 9: name=\"autogenerated1\">The Women of the All-American Girls Professional Baseball League</ref>\n",
      "Creating context for human_text:  20%|██        | 7984/39495 [04:07<16:18, 32.21it/s]WARNING:root:Closing tag without opening in sentence 3: Ryerson Press, 1951, p. 520.</ref>.\n",
      "Creating context for human_text:  21%|██▏       | 8436/39495 [04:21<15:05, 34.29it/s]WARNING:root:Closing tag without opening in sentence 0: Maggie Anderson (born September 23, 1948) is an American poet and editor with roots in Appalachia.<ref Name=\"News\">{{Cite web |url=http://www.newletters.org/PDFs/Anderson%20Interview.pdf |title='Newsletters interview  |access-date=2011-06-28 |archive-url=https://web.archive.org/web/20120722223820/http://www.newletters.org/PDFs/Anderson%20Interview.pdf |archive-date=2012-07-22 |url-status=dead }}</ref>.\n",
      "Creating context for human_text:  21%|██▏       | 8436/39495 [04:21<16:02, 32.27it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, NoneType found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m tqdm(df_wiki\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(df_wiki), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating context for \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mauthor):\n\u001b[1;32m      7\u001b[0m     text \u001b[38;5;241m=\u001b[39m row[author]\n\u001b[0;32m----> 8\u001b[0m     context_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcontext_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     context \u001b[38;5;241m=\u001b[39m context_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m     df_wiki\u001b[38;5;241m.\u001b[39mat[index, author\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_prev-3-sen-context\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m context\n",
      "File \u001b[0;32m~/Documents/GitHub/NLP-Authorship-Attribution/PrepareSentenceContext.py:26\u001b[0m, in \u001b[0;36mPrepareSentenceContext.__call__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/NLP-Authorship-Attribution/PrepareSentenceContext.py:85\u001b[0m, in \u001b[0;36mPrepareSentenceContext.parse_sentences\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     83\u001b[0m         context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(previous_3)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m         context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprevious_3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     previous \u001b[38;5;241m=\u001b[39m sent_text\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, NoneType found"
     ]
    }
   ],
   "source": [
    "# Here we create the the context for each df and author\n",
    "context_parser = PrepareSentenceContext(context_policy='previous-3-sentences')\n",
    "\n",
    "for author in ['human_text', 'llama', 'falcon']:\n",
    "    df_wiki[author+\"_prev-3-sen-context\"] = \"\"\n",
    "    for index, row in tqdm(df_wiki.iterrows(), total=len(df_wiki), desc=\"Creating context for \"+author):\n",
    "        text = row[author]\n",
    "        context_dict = context_parser(text)\n",
    "        context = context_dict['context']\n",
    "        df_wiki.at[index, author+\"_prev-3-sen-context\"] = context\n",
    "\n",
    "# for author in ['human_text', 'llama', 'falcon']:\n",
    "#     df_news[author+\"_prev-3-sen-context\"] = \"\"\n",
    "#     for index, row in tqdm(df_news.iterrows(), total=len(df_news), desc=\"Creating context for \"+author):\n",
    "#         text = row[author]\n",
    "#         context_dict = context_parser(text)\n",
    "#         context = context_dict['context']\n",
    "#         df_news.at[index, author+\"_prev-3-sen-context\"] = context\n",
    "\n",
    "# for author in ['human_text', 'llama', 'falcon']:\n",
    "#     df_abstracts[author+\"_prev-3-sen-context\"] = \"\"\n",
    "#     for index, row in tqdm(df_abstracts.iterrows(), total=len(df_abstracts), desc=\"Creating context for \"+author):\n",
    "#         text = row[author]\n",
    "#         context_dict = context_parser(text)\n",
    "#         context = context_dict['context']\n",
    "#         df_abstracts.at[index, author+\"_prev-3-sen-context\"] = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maggie Anderson (born September 23, 1948) is an American poet and editor with roots in Appalachia.<ref Name=\"News\">{{Cite web |url=http://www.newletters.org/PDFs/Anderson%20Interview.pdf |title='Newsletters interview  |access-date=2011-06-28 |archive-url=https://web.archive.org/web/20120722223820/http://www.newletters.org/PDFs/Anderson%20Interview.pdf |archive-date=2012-07-22 |url-status=dead }}</ref> Education and beginning of career\n",
      "Anderson attended West Virginia Wesleyan College from 1966–68 and earned a bachelor's degree in English, with high honors, from West Virginia University in 1970. Her M.A. in English (Creative Writing) in 1973 and an M.S.W. in 1977 were also from WVU.  She worked as a rehabilitation counselor for blind and visually impaired clients at the West Virginia Rehabilitation Center from 1973-77.  Beginning in 1979, she worked as poet-in-residence for ten years, in schools, senior centers, correctional facilities and libraries in West Virginia, Ohio, and Pennsylvania.   She has served as visiting writer at several universities, including the University of Pittsburgh, the University of Oregon, the Pennsylvania State University, Hamilton College, and West Virginia University.  In addition to her travels in the United States, Anderson has lived in Denmark (1992–1993) and traveled extensively throughout western and eastern Europe, Russia, and Scandinavia.\n"
     ]
    }
   ],
   "source": [
    "print(df_wiki.iloc[8436]['human_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV for log-ppx response calculation\n",
    "df_wiki.iloc[0:4].to_csv('wiki_dataset.csv', index=False)\n",
    "df_news.iloc[0:4].to_csv('news_dataset.csv', index=False)\n",
    "df_abstracts.iloc[0:4].to_csv('abstracts_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
