{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adamvinestock/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import torch\n",
    "\n",
    "nltk.download('punkt')\n",
    "pd.set_option('display.max_columns', None)  # Show all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HuggingFace wiki_intro_long dataset\n",
    "hf_wiki_dataset = load_dataset('alonkipnis/wiki-intro-long', split='train')\n",
    "df_wiki = pd.DataFrame(hf_wiki_dataset)\n",
    "\n",
    "# Add columns for Llama2 and Falcon7B model outputs\n",
    "df_wiki['human_len'] = None\n",
    "df_wiki['gpt_len'] = None\n",
    "df_wiki['Llama2'], df_wiki['Llama2_len'] = None, None\n",
    "df_wiki['Falcon7B'], df_wiki['Falcon7B_len'] = None, None\n",
    "\n",
    "df_wiki.rename(columns={\n",
    "    'wiki_intro': 'human_text',\n",
    "    # 'wiki_intro_len': 'human_len',\n",
    "    'generated_intro': 'gpt'\n",
    "    }, inplace=True)\n",
    "\n",
    "columns_to_drop = ['prompt_tokens', 'generated_text', 'generated_intro_len']\n",
    "df_wiki.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "new_order = [\n",
    "    'id', 'url', 'title', 'title_len', 'prompt',\n",
    "    'human_text', 'human_len',\n",
    "    'gpt', 'gpt_len',\n",
    "    'Llama2', 'Llama2_len',\n",
    "    'Falcon7B', 'Falcon7B_len']\n",
    "\n",
    "df_wiki = df_wiki[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'url', 'title', 'title_len', 'prompt', 'human_text', 'human_len',\n",
      "       'gpt', 'gpt_len', 'Llama2', 'Llama2_len', 'Falcon7B', 'Falcon7B_len'],\n",
      "      dtype='object')\n",
      "39495\n"
     ]
    }
   ],
   "source": [
    "print(df_wiki.columns)\n",
    "print(df_wiki.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HuggingFace news dataset\n",
    "hf_news_dataset = load_dataset('alonkipnis/news-chatgpt-long', split='train')\n",
    "df_news = pd.DataFrame(hf_news_dataset)\n",
    "\n",
    "df_news.rename(columns={\n",
    "    'article': 'human_text',\n",
    "    'chatgpt': 'gpt'\n",
    "}, inplace=True)\n",
    "\n",
    "df_news['human_len'], df_news['gpt_len'] = None, None\n",
    "df_news['Llama2'], df_news['Llama2_len'] = None, None\n",
    "df_news['Falcon7B'], df_news['Falcon7B_len'] = None, None\n",
    "df_news['prompt'] = None\n",
    "\n",
    "\n",
    "columns_to_drop = ['highlights']  \n",
    "df_news.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "new_order = [\n",
    "    'id', 'prompt',\n",
    "    'human_text', 'human_len',\n",
    "    'gpt', 'gpt_len',\n",
    "    'Llama2', 'Llama2_len',\n",
    "    'Falcon7B', 'Falcon7B_len'\n",
    "]\n",
    "\n",
    "df_news = df_news[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'prompt', 'human_text', 'human_len', 'gpt', 'gpt_len', 'Llama2',\n",
      "       'Llama2_len', 'Falcon7B', 'Falcon7B_len'],\n",
      "      dtype='object')\n",
      "13025\n"
     ]
    }
   ],
   "source": [
    "print(df_news.columns)\n",
    "print(df_news.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HuggingFace research absracts dataset\n",
    "hf_abstracts_dataset = load_dataset('NicolaiSivesind/ChatGPT-Research-Abstracts', split='train')\n",
    "df_abstracts = pd.DataFrame(hf_abstracts_dataset)\n",
    "\n",
    "df_abstracts.rename(columns={\n",
    "    'real_abstract': 'human_text',\n",
    "    'real_word_count': 'human_len',\n",
    "    'generated_abstract': 'gpt',\n",
    "    'generated_word_count': 'gpt_len'\n",
    "}, inplace=True)\n",
    "\n",
    "df_abstracts['Llama2'], df_abstracts['Llama2_len'] = None, None\n",
    "df_abstracts['Falcon7B'], df_abstracts['Falcon7B_len'] = None, None\n",
    "df_abstracts['prompt'] = None\n",
    "\n",
    "new_order = [\n",
    "    'title', 'prompt',\n",
    "    'human_text', 'human_len',\n",
    "    'gpt', 'gpt_len',\n",
    "    'Llama2', 'Llama2_len',\n",
    "    'Falcon7B', 'Falcon7B_len'\n",
    "]\n",
    "\n",
    "df_abstracts = df_abstracts[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'prompt', 'human_text', 'human_len', 'gpt', 'gpt_len',\n",
      "       'Llama2', 'Llama2_len', 'Falcon7B', 'Falcon7B_len'],\n",
      "      dtype='object')\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(df_abstracts.columns)\n",
    "print(df_abstracts.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_and_sentences(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the text into words and sentences using nltk \n",
    "    Returns a tuple of (n_words,n_sentences)\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    return (len(words), len(sentences))\n",
    "\n",
    "def generate_text(prompt, model, tokenizer, max_length=500):\n",
    "    \"\"\"\n",
    "    Encodes the prompt using the model tokenizer\n",
    "    Returns the generated text, word count and sentence count\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set padding token to eos\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length)\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    attention_mask = torch.fliplr(attention_mask)    # Adjusting the attention mask for left padding\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'], \n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=max_length+100, \n",
    "        num_return_sequences=1)\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    n_words, n_sentences = count_words_and_sentences(generated_text)\n",
    "    return generated_text, n_words, n_sentences\n",
    "\n",
    "def create_wiki_prompt(row):\n",
    "    \"\"\"\n",
    "    Creates the wiki dataset prompt using the title and first 7 words written by humans\n",
    "    \"\"\"\n",
    "    first_few_words = ' '.join(row['human_text'].split()[:7]) \n",
    "    prompt = f\"Write a Wikipedia-style intro covering the topic '{row['title']}', it should be detailed and span approximately {row['human_len'][1]} sentences long. {first_few_words}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate length of human text with tuple(word_count, sentence_count)\n",
    "df_wiki['human_len'] = df_wiki['human_text'].apply(count_words_and_sentences)\n",
    "df_news['human_len'] = df_news['human_text'].apply(count_words_and_sentences)\n",
    "df_abstracts['human_len'] = df_abstracts['human_text'].apply(count_words_and_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_news['human_len'].iloc[0:5]\n",
    "# for text in df_news['human_text'].iloc[0:5]:\n",
    "#     print(text)\n",
    "#     print(\"\\n---\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki['prompt'] = df_wiki.iloc[0:10].apply(create_wiki_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# Load gpt2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.padding_side = 'left'\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "for index, row in df_wiki.head(2).iterrows():\n",
    "    prompt = row['prompt']\n",
    "    # generated_text, (word_count, sentence_count) = generate_text(prompt, model, tokenizer, max_length=500)\n",
    "    generated_text, word_count, sent_count = generate_text(prompt, model, tokenizer, max_length=500)\n",
    "    df_wiki.at[index, 'gpt'] = generated_text\n",
    "    df_wiki.at[index, 'gpt_len'] = [(word_count,sent_count)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Export to CSV for external analysis\n",
    "df_wiki.iloc[0:2].to_csv('generated_text_comparison.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
