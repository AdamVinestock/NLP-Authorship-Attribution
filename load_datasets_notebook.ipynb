{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch  \n",
    "# !pip install datasets  \n",
    "# !hostname\n",
    "# !pip install --upgrade transformers\n",
    "# ! pip install spacy\n",
    "# !pip install ipywidgets\n",
    "# !pip install --upgrade transformers \"numpy<2\" \"pyarrow<14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep  7 19:28:45 2024       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  Tesla V100-PCIE-16GB           Off |   00000001:00:00.0 Off |                  Off |\r\n",
      "| N/A   29C    P0             25W /  250W |     165MiB /  16384MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|    0   N/A  N/A    304226      G   /usr/lib/xorg/Xorg                            164MiB |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# !nvcc --version\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 19:28:49.643365: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-07 19:28:50.583408: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64_lin/gcc4.7:/opt/intel/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64_lin::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n",
      "2024-09-07 19:28:50.583577: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64_lin/gcc4.7:/opt/intel/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64_lin::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n",
      "2024-09-07 19:28:50.583591: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[nltk_data] Downloading package punkt to /home/treuser1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import set_seed\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from PrepareSentenceContext import PrepareSentenceContext\n",
    "\n",
    "nltk.download('punkt')\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.1+cu118\n",
      "CUDA is available: True\n",
      "CUDA version: 11.8\n"
     ]
    }
   ],
   "source": [
    "# Check current PyTorch and CUDA availability\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook_login()\n",
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HuggingFace wiki_intro_long dataset\n",
    "hf_wiki_dataset = load_dataset('alonkipnis/wiki-intro-long', split='train')\n",
    "df_wiki = pd.DataFrame(hf_wiki_dataset)\n",
    "\n",
    "# Add columns for Llama2 and Falcon7B model outputs\n",
    "df_wiki['human_len'] = None\n",
    "df_wiki['gpt_len'] = None\n",
    "df_wiki['Llama3.1'], df_wiki['Llama3.1_len'] = None, None\n",
    "df_wiki['Falcon'], df_wiki['Falcon_len'] = None, None\n",
    "\n",
    "df_wiki.rename(columns={\n",
    "    'wiki_intro': 'human_text',\n",
    "    # 'wiki_intro_len': 'human_len',\n",
    "    'generated_intro': 'gpt'\n",
    "    }, inplace=True)\n",
    "\n",
    "columns_to_drop = ['prompt_tokens', 'generated_text', 'generated_intro_len']\n",
    "df_wiki.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "new_order = [\n",
    "    'id', 'url', 'title', 'title_len', 'prompt',\n",
    "    'human_text', 'human_len',\n",
    "    'gpt', 'gpt_len',\n",
    "    'Llama3.1', 'Llama3.1_len',\n",
    "    'Falcon', 'Falcon_len']\n",
    "\n",
    "df_wiki = df_wiki[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'url', 'title', 'title_len', 'prompt', 'human_text', 'human_len',\n",
      "       'gpt', 'gpt_len', 'Llama2', 'Llama2_len', 'Llama3.1', 'Llama3.1_len',\n",
      "       'Falcon', 'Falcon_len'],\n",
      "      dtype='object')\n",
      "39495\n"
     ]
    }
   ],
   "source": [
    "print(df_wiki.columns)\n",
    "print(df_wiki.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HuggingFace news dataset\n",
    "hf_news_dataset = load_dataset('alonkipnis/news-chatgpt-long', split='train')\n",
    "df_news = pd.DataFrame(hf_news_dataset)\n",
    "\n",
    "df_news.rename(columns={\n",
    "    'article': 'human_text',\n",
    "    'chatgpt': 'gpt'\n",
    "}, inplace=True)\n",
    "\n",
    "df_news['human_len'], df_news['gpt_len'] = None, None\n",
    "df_news['Llama3.1'], df_news['Llama3.1_len'] = None, None\n",
    "df_news['Falcon'], df_news['Falcon_len'] = None, None\n",
    "df_news['prompt'] = None\n",
    "\n",
    "new_order = [\n",
    "    'id', 'highlights', 'prompt',\n",
    "    'human_text', 'human_len',\n",
    "    'gpt', 'gpt_len',\n",
    "    'Llama3.1', 'Llama3.1_len',\n",
    "    'Falcon', 'Falcon_len'\n",
    "]\n",
    "\n",
    "df_news = df_news[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'highlights', 'prompt', 'human_text', 'human_len', 'gpt',\n",
      "       'gpt_len', 'Llama2', 'Llama2_len', 'Llama3.1', 'Llama3.1_len', 'Falcon',\n",
      "       'Falcon_len'],\n",
      "      dtype='object')\n",
      "13025\n"
     ]
    }
   ],
   "source": [
    "print(df_news.columns)\n",
    "print(df_news.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HuggingFace research absracts dataset\n",
    "hf_abstracts_dataset = load_dataset('NicolaiSivesind/ChatGPT-Research-Abstracts', split='train')\n",
    "df_abstracts = pd.DataFrame(hf_abstracts_dataset)\n",
    "\n",
    "df_abstracts.rename(columns={\n",
    "    'real_abstract': 'human_text',\n",
    "    'real_word_count': 'human_len',\n",
    "    'generated_abstract': 'gpt',\n",
    "    'generated_word_count': 'gpt_len'\n",
    "}, inplace=True)\n",
    "\n",
    "df_abstracts['Llama3.1'], df_abstracts['Llama3.1_len'] = None, None\n",
    "df_abstracts['Falcon'], df_abstracts['Falcon_len'] = None, None\n",
    "df_abstracts['prompt'] = None\n",
    "\n",
    "new_order = [\n",
    "    'title', 'prompt',\n",
    "    'human_text', 'human_len',\n",
    "    'gpt', 'gpt_len',\n",
    "    'Llama3.1', 'Llama3.1_len',\n",
    "    'Falcon', 'Falcon_len'\n",
    "]\n",
    "\n",
    "df_abstracts = df_abstracts[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'prompt', 'human_text', 'human_len', 'gpt', 'gpt_len',\n",
      "       'Llama2', 'Llama2_len', 'Llama3.1', 'Llama3.1_len', 'Falcon',\n",
      "       'Falcon_len'],\n",
      "      dtype='object')\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(df_abstracts.columns)\n",
    "print(df_abstracts.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_and_sentences(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the text into words and sentences using nltk \n",
    "    Returns a tuple of (n_words,n_sentences)\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    return (len(words), len(sentences))\n",
    "\n",
    "def create_wiki_prompt(row):\n",
    "    \"\"\"\n",
    "    Creates the wiki dataset prompt using the title and first 7 words written by humans\n",
    "    \"\"\"\n",
    "    first_few_words = ' '.join(row['human_text'].split()[:7]) \n",
    "    # prompt = (\n",
    "    #     f\"Your role is a Wikipedia contributor.\\n\"\n",
    "    #     f\"Your task is to compose a detailed Wikipedia-style introduction for the topic '{row['title']}'.\\n\"\n",
    "    #     f\"**The introduction should be exactly {row['human_len'][1]} sentences long.**\\n\"\n",
    "    #     f\"Begin with a clear definition, followed by key details and context essential for understanding the topic.\\n\"\n",
    "    #     f\"Do not include section headings, bullet points, or editing instructions.\\n\\n\"\n",
    "    #     f\"Introduction: {first_few_words}\"\n",
    "    # )\n",
    "    prompt = (\n",
    "    f\"### | Instruction | ###\\n\"\n",
    "    f\"Your role is a Wikipedia contributor.\\n\"\n",
    "    f\"Your task is to compose a detailed Wikipedia-style introduction for the topic '{row['title']}'.\\n\"\n",
    "    f\"The introduction should be at least {row['human_len'][0]} words long and {row['human_len'][1]} sentences long.\\n\"\n",
    "    f\"Begin with a clear definition, followed by key details and context essential for understanding the topic.\\n\"\n",
    "    f\"Do not include headings or editorial notes.\\n\"\n",
    "    f\"### | Response | ###\\n\"\n",
    "    f\"{first_few_words}\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def create_news_prompt(row):\n",
    "    \"\"\"\n",
    "    Creates the news dataset prompt using the first 15 words written by humans, and the article highlights\n",
    "    \"\"\"\n",
    "    first_few_words = ' '.join(row['human_text'].split()[:15]) \n",
    "    highlights = row['highlights'] \n",
    "    # prompt = (\n",
    "    #     f\"Your role is a professional news journalist.\\n\"\n",
    "    #     f\"Your task is to write a detailed news article based on the given highlights.\\n\"\n",
    "    #     f\"**The article should be exactly {row['human_len'][1]} sentences long.**\\n\"\n",
    "    #     f\"Incorporate the following key highlights: {highlights}\\n\"\n",
    "    #     f\"Begin with the most important details, followed by context and supporting information.\\n\"\n",
    "    #     f\"Do not include headings, bullet points, or editorial notes.\\n\\n\"\n",
    "    #     f\"Article: {first_few_words}\"\n",
    "    # )\n",
    "    prompt = (\n",
    "    f\"### | Instruction | ###\\n\"\n",
    "    f\"Your role is a professional news journalist.\\n\"\n",
    "    f\"Your task is to write a detailed news article based on the given highlights.\\n\"\n",
    "    f\"The article should be at least {row['human_len'][0]} words long and {row['human_len'][1]} sentences long.\\n\"\n",
    "    f\"Incorporate the following key highlights: {highlights}\\n\"\n",
    "    f\"Begin with the most important details, followed by context and supporting information.\\n\"\n",
    "    f\"Do not include headings or editorial notes.\\n\"\n",
    "    f\"### | Response | ###\\n\"\n",
    "    f\"{first_few_words}\\n\"\n",
    ")\n",
    "    return prompt\n",
    "\n",
    "def create_abstracts_prompt(row):\n",
    "    \"\"\"\n",
    "    Creates the abstracts dataset prompt using the title and first 15 words written by humans\n",
    "    \"\"\"\n",
    "    first_few_words = ' '.join(row['human_text'].split()[:15]) \n",
    "    # prompt = (\n",
    "    #     f\"Your role is a scientist writing a research abstract for the paper titled '{row['title']}'.\\n\"\n",
    "    #     f\"Your task is to write a concise, detailed abstract for this paper.\\n\"\n",
    "    #     f\"**The abstract should be exactly {row['human_len'][1]} sentences long.**\\n\"\n",
    "    #     f\"Begin with a clear statement of the research question or problem, followed by methodology, findings, and implications.\\n\"\n",
    "    #     f\"Do not include section headings, bullet points, or additional instructions.\\n\\n\"\n",
    "    #     f\"Abstract: {first_few_words}\"\n",
    "    # )\n",
    "    prompt = (\n",
    "    f\"### | Instruction | ###\\n\"\n",
    "    f\"Your role is a scientist writing a research abstract for the paper titled '{row['title']}'.\\n\"\n",
    "    f\"Your task is to write a concise, detailed abstract for this paper.\\n\"\n",
    "    f\"The abstract should be at least {row['human_len'][0]} words long and {row['human_len'][1]} sentences long.\\n\"\n",
    "    f\"Begin with a clear statement of the research question or problem, followed by methodology, findings, and implications.\\n\"\n",
    "    f\"Do not include headings or editorial notes.\\n\"\n",
    "    f\"### | Response | ###\\n\"\n",
    "    f\"{first_few_words}\\n\"\n",
    ")\n",
    "    return prompt\n",
    "\n",
    "def generate_text_gpt2xl(prompt, model, tokenizer, max_length=1024):\n",
    "    \"\"\"\n",
    "    Encodes the prompt using the model tokenizer - max context windows of 1024, left padding\n",
    "    Returns the generated text, word count and sentence count\n",
    "    \"\"\"\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length) # Prepare input encoding with padding and truncation\n",
    "    attention_mask = torch.fliplr(inputs['attention_mask'])                                                       # Adjust attention mask for left padding \n",
    "    max_new_tokens = 1024 - inputs['input_ids'].shape[1]                                                          # gpt2 is limited to generating 1024 token including prompt\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'], \n",
    "        attention_mask=attention_mask,  \n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    n_words, n_sentences = count_words_and_sentences(generated_text)\n",
    "    return generated_text, n_words, n_sentences\n",
    "\n",
    "def generate_text_gpt2xl_v2(prompt, model, tokenizer):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True)\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=1024,  # Set to the maximum length of the model\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    n_words, n_sentences = count_words_and_sentences(generated_text)\n",
    "    return generated_text, n_words, n_sentences\n",
    "\n",
    "\n",
    "\n",
    "def generate_text_llama2(prompt, model, tokenizer, max_length=500):\n",
    "    \"\"\"\n",
    "    Encodes the prompt using the model tokenizer\n",
    "    Returns the generated text, word count and sentence count\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to('cuda')\n",
    "        model = model.to('cuda')\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'], \n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        do_sample=True,       # Enable sampling to generate more diverse responses\n",
    "        # temperature=0.9,    # Slightly randomize the outputs to prevent repetition\n",
    "        # top_k=50,           # Consider top 50 tokens for sampling at each step\n",
    "        # top_p=0.95,          \n",
    "        top_p = 0.9,\n",
    "        num_beams = 5,\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    start_index = min((generated_text.find(key) for key in [\"Introduction:\", \"Article:\", \"Abstract:\"] if generated_text.find(key) != -1), default=0)\n",
    "    generated_text = generated_text[start_index:].strip()\n",
    "    n_words, n_sentences = count_words_and_sentences(generated_text)\n",
    "    return generated_text, n_words, n_sentences\n",
    "\n",
    "def generate_text_llama3(prompt, model, tokenizer, length_params):\n",
    "    \"\"\"\n",
    "    Encodes the prompt using the model tokenizer\n",
    "    Returns the generated text, word count and sentence count\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=512)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to('cuda')\n",
    "        model = model.to('cuda')\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'], \n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        num_return_sequences=1,\n",
    "        max_new_tokens=length_params[\"max_length\"],\n",
    "        do_sample=False,     # Enable sampling to generate more diverse responses\n",
    "        # temperature=0.5,    # Slightly randomize the outputs to prevent repetition\n",
    "        # top_k=20,           # Consider top 50 tokens for sampling at each step      \n",
    "        # top_p = 0.9,\n",
    "        num_beams = 5,     # cant run with v100 16gGB\n",
    "        no_repeat_ngram_size=5,\n",
    "        repetition_penalty=1.5,\n",
    "        length_penalty=0.9,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    match = re.search(r\"### \\| Response \\| ###\\s*(.*)\", generated_text, re.DOTALL)\n",
    "    if match:\n",
    "        response_text = match.group(1).strip()\n",
    "    else:\n",
    "        print(\"Response delimiter not found in generated text!\")\n",
    "        response_text = generated_text\n",
    "    n_words, n_sentences = count_words_and_sentences(response_text)\n",
    "    return response_text, n_words, n_sentences\n",
    "\n",
    "\n",
    "def generate_text_falcon(prompt, model, tokenizer, length_params):\n",
    "    \"\"\"\n",
    "    Encodes the prompt using the model tokenizer\n",
    "    Returns the generated text, word count and sentence count\n",
    "    \"\"\"\n",
    "    \n",
    "    # Adjust tokenizer padding for decoding\n",
    "    tokenizer.padding_side = 'left'\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token or '[PAD]'\n",
    "\n",
    "    # Encode the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to('cuda')\n",
    "        model = model.to('cuda')\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=length_params[\"max_length\"], \n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=4,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        do_sample=False,\n",
    "        # top_p=0.9,\n",
    "        # temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    match = re.search(r\"### \\| Response \\| ###\\s*(.*)\", generated_text, re.DOTALL)\n",
    "    if match:\n",
    "        response_text = match.group(1).strip()\n",
    "    else:\n",
    "        print(\"Response delimiter not found in generated text!\")\n",
    "        response_text = generated_text \n",
    "    n_words, n_sentences = count_words_and_sentences(response_text)\n",
    "    return response_text, n_words, n_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate length of human text with tuple(word_count, sentence_count)\n",
    "df_wiki['human_len'] = df_wiki['human_text'].apply(count_words_and_sentences)\n",
    "df_news['human_len'] = df_news['human_text'].apply(count_words_and_sentences)\n",
    "df_abstracts['human_len'] = df_abstracts['human_text'].apply(count_words_and_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate length of GPT text with tuple(word_count, sentence_count)\n",
    "df_wiki['gpt_len'] = df_wiki['gpt'].apply(count_words_and_sentences)\n",
    "df_news['gpt_len'] = df_news['gpt'].apply(count_words_and_sentences)\n",
    "df_abstracts['gpt_len'] = df_abstracts['gpt'].apply(count_words_and_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prompts\n",
    "df_wiki['prompt'] = df_wiki.iloc[0:10].apply(create_wiki_prompt, axis=1)\n",
    "df_news['prompt'] = df_news.iloc[0:10].apply(create_news_prompt, axis=1)\n",
    "df_abstracts['prompt'] = df_abstracts.iloc[0:10].apply(create_abstracts_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia - Average sentences: 9.162501582478795\n",
      "Wikipedia - Median sentences: 9.0\n",
      "Wikipedia - Minimum sentences: 1\n",
      "Wikipedia - Maximum sentences: 71\n",
      "News - Average sentences: 22.985335892514396\n",
      "News - Median sentences: 22.0\n",
      "News - Minimum sentences: 9\n",
      "News - Maximum sentences: 126\n",
      "Abstracts - Average sentences: 8.0332\n",
      "Abstracts - Median sentences: 8.0\n",
      "Abstracts - Minimum sentences: 1\n",
      "Abstracts - Maximum sentences: 35\n"
     ]
    }
   ],
   "source": [
    "# domain articles length stats for tuning max length generation \n",
    "def calc_sentence_stats(df, task_name):\n",
    "    sentence_counts = [t[1] for t in df['human_len']]\n",
    "    avg_sentences = pd.Series(sentence_counts).mean()\n",
    "    median_sentences = pd.Series(sentence_counts).median()\n",
    "    min_sentences = pd.Series(sentence_counts).min()\n",
    "    max_sentences = pd.Series(sentence_counts).max()\n",
    "    \n",
    "    print(f\"{task_name} - Average sentences: {avg_sentences}\")\n",
    "    print(f\"{task_name} - Median sentences: {median_sentences}\")\n",
    "    print(f\"{task_name} - Minimum sentences: {min_sentences}\")\n",
    "    print(f\"{task_name} - Maximum sentences: {max_sentences}\")\n",
    "\n",
    "calc_sentence_stats(df_wiki, 'Wikipedia')\n",
    "calc_sentence_stats(df_news, 'News')\n",
    "calc_sentence_stats(df_abstracts, 'Abstracts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length_params(task_type):\n",
    "    if task_type == \"wikipedia\":\n",
    "        return {\"max_length\": 512}  \n",
    "    elif task_type == \"news\":\n",
    "        return {\"max_length\": 1024}  # For longer news articles\n",
    "    elif task_type == \"abstract\":\n",
    "        return {\"max_length\": 512} \n",
    "    else:\n",
    "        return {\"max_length\": 1024} \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load gpt2-xl model and tokenizer\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
    "# tokenizer.padding_side = 'left'  # Ensure padding from the left for gpt2\n",
    "# model = GPT2LMHeadModel.from_pretrained('gpt2-xl')\n",
    "# model.eval()\n",
    "\n",
    "# for index, row in df_wiki.head(1).iterrows():  \n",
    "#     prompt = row['prompt'] \n",
    "#     generated_text, word_count, sent_count = generate_text_gpt2xl_v2(prompt, model, tokenizer)\n",
    "#     df_wiki.at[index, 'gpt'] = generated_text\n",
    "#     df_wiki.at[index, 'gpt_len'] = [(word_count, sent_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #### Toy example to test llama3.1 8B model and tokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "# # model = model.half() # required for 16GB gpu (8B 32float = 32gb) so reduce to 16float\n",
    "# if torch.cuda.is_available():\n",
    "#     model.cuda()  \n",
    "# model.eval()\n",
    "\n",
    "# prompt = (\n",
    "#     f\"Your role is a Wikipedia contributor. \"\n",
    "#     f\"Compose a Wikipedia-style introduction for the topic 'Moluccans'. \"\n",
    "#     f\"Start with a clear definition, followed by key details and context that is essential for understanding the subject. \"\n",
    "#     f\"Ensure the introduction is detailed and spans approximately 11 sentences long. \"\n",
    "#     f\"Introduction: Moluccans are the Austronesian-speaking and Papuan-speaking ethnic\"\n",
    "# )\n",
    "\n",
    "# inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=4096)\n",
    "# if torch.cuda.is_available():\n",
    "#     inputs = inputs.to('cuda')\n",
    "#     model = model.to('cuda')\n",
    "# output_ids = model.generate(\n",
    "#     input_ids=inputs['input_ids'], \n",
    "#     attention_mask=inputs['attention_mask'],\n",
    "#     do_sample=True,       # Enable sampling to generate more diverse responses\n",
    "#     # temperature=0.9,    # Slightly randomize the outputs to prevent repetition\n",
    "#     # top_k=50,           # Consider top 50 tokens for sampling at each step\n",
    "#     # top_p=0.95,          \n",
    "#     top_p = 0.9,\n",
    "#     # num_beams = 5,\n",
    "#     temperature=0.7,\n",
    "#     num_return_sequences=1,\n",
    "#     max_length = 512\n",
    "# )\n",
    "\n",
    "# generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "# start_index = -1\n",
    "# for keyword in [\"Introduction:\", \"Article:\", \"Abstract:\"]:\n",
    "#     start_index = generated_text.find(keyword)\n",
    "#     if start_index != -1:\n",
    "#         start_index += len(keyword)\n",
    "#         break  # Stop once we find a valid keyword\n",
    "\n",
    "# if start_index != -1:\n",
    "#     generated_text = generated_text[start_index:].strip()\n",
    "# else:\n",
    "#     print(\"Keyword not found in generated text!\")\n",
    "\n",
    "# n_words, n_sentences = count_words_and_sentences(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Toy example to test model and tokenizer\n",
    "# # Load llama2 7B tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "# if torch.cuda.is_available():\n",
    "#     model.cuda()  \n",
    "# model.eval()\n",
    "\n",
    "# prompt = (\n",
    "#     f\"Your role is a Wikipedia contributor. \"\n",
    "#     f\"Compose a Wikipedia-style introduction for the topic ''Moluccans''. \"\n",
    "#     f\"Start with a clear definition, followed by key details and context that is essential for understanding the subject. \"\n",
    "#     f\"Ensure the introduction is detailed and spans approximately 11 sentences long. \"\n",
    "#     f\"Introduction: Moluccans are the Austronesian-speaking and Papuan-speaking ethnic\"\n",
    "# )\n",
    "\n",
    "# inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=4096)\n",
    "# if torch.cuda.is_available():\n",
    "#     inputs = inputs.to('cuda')\n",
    "#     model = model.to('cuda')\n",
    "# output_ids = model.generate(\n",
    "#     input_ids=inputs['input_ids'], \n",
    "#     attention_mask=inputs['attention_mask'],\n",
    "#     do_sample=True,       # Enable sampling to generate more diverse responses\n",
    "#     # temperature=0.9,    # Slightly randomize the outputs to prevent repetition\n",
    "#     # top_k=50,           # Consider top 50 tokens for sampling at each step\n",
    "#     # top_p=0.95,          \n",
    "#     top_p = 0.9,\n",
    "#     num_beams = 5,\n",
    "#     temperature=0.7,\n",
    "#     num_return_sequences=1\n",
    "# )\n",
    "# generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "# n_words, n_sentences = count_words_and_sentences(generated_text)\n",
    "\n",
    "# ##### Toy example to test model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Toy example to test model and tokenizer\n",
    "# Load falcon 7B tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\n",
    "# if torch.cuda.is_available():\n",
    "#     model.cuda()  \n",
    "# model.eval()\n",
    "\n",
    "# tokenizer.padding_side = 'left'\n",
    "# if tokenizer.pad_token is None:\n",
    "#     if tokenizer.eos_token:\n",
    "#         tokenizer.pad_token = tokenizer.eos_token\n",
    "#     else:\n",
    "#         tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# # Encode the prompt to tensor of input ids\n",
    "# inputs = tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=4096)\n",
    "# if torch.cuda.is_available():\n",
    "#     inputs = inputs.to('cuda')\n",
    "#     model = model.to('cuda')\n",
    "\n",
    "# output_ids = model.generate(\n",
    "#     input_ids=inputs['input_ids'],\n",
    "#     attention_mask=inputs['attention_mask'],\n",
    "#     max_length=4096, \n",
    "#     num_return_sequences=1,\n",
    "#     no_repeat_ngram_size=4,  # Prevents the model from repeating the same 4-gram\n",
    "#     top_p=0.92,\n",
    "#     top_k=50,\n",
    "#     temperature=0.7\n",
    "# )\n",
    "\n",
    "# generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True) \n",
    "# n_words, n_sentences = count_words_and_sentences(generated_text)\n",
    "\n",
    "# ##### Toy example to test model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9e9ace20a4465f8b945a12173b42e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Wiki Dataset:   0%|                                                                                                                                                                                                        | 0/3 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating Wiki Dataset:  33%|████████████████████████████████████████████████████████████████                                                                                                                                | 1/3 [00:03<00:06,  3.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating Wiki Dataset:  67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                | 2/3 [00:25<00:14, 14.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating Wiki Dataset: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:40<00:00, 14.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating Wiki Dataset: 4it [00:51, 12.95s/it]                                                                                                                                                                                                             Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating Wiki Dataset: 5it [00:55,  9.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating Wiki Dataset: 6it [01:07, 10.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating Wiki Dataset: 7it [01:14,  9.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating Wiki Dataset: 8it [01:18,  7.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating Wiki Dataset: 9it [01:24,  7.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating Wiki Dataset: 10it [01:29,  9.00s/it]\n",
      "Generating News Dataset:   0%|                                                                                                                                                                                                        | 0/3 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating News Dataset:  33%|████████████████████████████████████████████████████████████████                                                                                                                                | 1/3 [00:08<00:16,  8.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating News Dataset:  67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                | 2/3 [00:19<00:09,  9.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating News Dataset: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:22<00:00,  6.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating News Dataset: 4it [00:23,  4.38s/it]                                                                                                                                                                                                             Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating News Dataset: 5it [00:26,  3.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating News Dataset: 6it [00:36,  6.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating News Dataset: 7it [00:48,  8.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating News Dataset: 8it [00:59,  8.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating News Dataset: 9it [01:17, 11.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating News Dataset: 10it [01:28,  8.82s/it]\n",
      "Generating Abstracts Dataset:   0%|                                                                                                                                                                                                   | 0/3 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating Abstracts Dataset:  33%|██████████████████████████████████████████████████████████████▎                                                                                                                            | 1/3 [00:08<00:17,  8.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating Abstracts Dataset:  67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                              | 2/3 [00:16<00:07,  7.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating Abstracts Dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:17<00:00,  4.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating Abstracts Dataset: 4it [00:39, 11.85s/it]                                                                                                                                                                                                        Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating Abstracts Dataset: 5it [00:45,  9.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating Abstracts Dataset: 6it [00:48,  7.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating Abstracts Dataset: 7it [01:02,  9.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating Abstracts Dataset: 8it [01:17, 11.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating Abstracts Dataset: 9it [01:24, 10.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Generating Abstracts Dataset: 10it [01:37,  9.75s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load llama3.1 8B tokenizer and model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "# model = model.half() # required for 16GB gpu, (8B 32 float = 32gb)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()  \n",
    "model.eval()\n",
    "\n",
    "# Generate text for the wiki dataset\n",
    "task_type = \"wikipedia\"  \n",
    "length_params = get_length_params(task_type)\n",
    "for index, row in tqdm(df_wiki.head(10).iterrows(), total=10, desc=\"Generating Wiki Dataset\"):\n",
    "    prompt = row['prompt']\n",
    "    generated_text, word_count, sent_count = generate_text_llama3(prompt, model, tokenizer, length_params)\n",
    "    df_wiki.at[index, 'Llama3.1'] = generated_text\n",
    "    df_wiki.at[index, 'Llama3.1_len'] = [(word_count, sent_count)]\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "# Generate text for the news dataset\n",
    "task_type = \"news\"  \n",
    "length_params = get_length_params(task_type)\n",
    "for index, row in tqdm(df_news.head(10).iterrows(), total=10, desc=\"Generating News Dataset\"):\n",
    "    prompt = row['prompt']\n",
    "    generated_text, word_count, sent_count = generate_text_llama3(prompt, model, tokenizer, length_params)\n",
    "    df_news.at[index, 'Llama3.1'] = generated_text\n",
    "    df_news.at[index, 'Llama3.1_len'] = [(word_count, sent_count)]\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "# Generate text for abstracts dataset\n",
    "task_type = \"abstract\"  \n",
    "length_params = get_length_params(task_type)\n",
    "for index, row in tqdm(df_abstracts.head(10).iterrows(), total=10, desc=\"Generating Abstracts Dataset\"):\n",
    "    prompt = row['prompt']\n",
    "    generated_text, word_count, sent_count = generate_text_llama3(prompt, model, tokenizer, length_params)\n",
    "    df_abstracts.at[index, 'Llama3.1'] = generated_text\n",
    "    df_abstracts.at[index, 'Llama3.1_len'] = [(word_count, sent_count)]\n",
    "    torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear LLaMA 3.1 8B model and tokenizer from GPU memory\n",
    "del model  \n",
    "del tokenizer \n",
    "torch.cuda.empty_cache()  # Clear the GPU cache\n",
    "gc.collect()  # Run Python garbage collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load falcon 7B tokenizer and model\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b\")\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()  \n",
    "model.eval()\n",
    "\n",
    "# generate text for wiki dataset\n",
    "task_type = \"wikipedia\"  \n",
    "length_params = get_length_params(task_type)\n",
    "for index, row in tqdm(df_wiki.head(10).iterrows(), total=10, desc=\"Generating Wiki Dataset\"):\n",
    "    prompt = row['prompt']\n",
    "    generated_text, word_count, sent_count = generate_text_falcon(prompt, model, tokenizer, length_params)\n",
    "    df_wiki.at[index, 'Falcon'] = generated_text\n",
    "    df_wiki.at[index, 'Falcon_len'] = [(word_count, sent_count)]\n",
    "\n",
    "# generate text for news dataset\n",
    "task_type = \"news\"  \n",
    "length_params = get_length_params(task_type)\n",
    "for index, row in tqdm(df_news.head(10).iterrows(), total=10, desc=\"Generating News Dataset\"):\n",
    "    prompt = row['prompt']\n",
    "    generated_text, word_count, sent_count = generate_text_falcon(prompt, model, tokenizer, length_params)\n",
    "    df_news.at[index, 'Falcon'] = generated_text\n",
    "    df_news.at[index, 'Falcon_len'] = [(word_count, sent_count)]\n",
    "\n",
    "# generate text for abstracts dataset\n",
    "task_type = \"abstract\"  \n",
    "length_params = get_length_params(task_type)\n",
    "for index, row in tqdm(df_abstracts.head(10).iterrows(), total=10, desc=\"Generating Abstracts Dataset\"):\n",
    "    prompt = row['prompt']\n",
    "    generated_text, word_count, sent_count = generate_text_falcon(prompt, model, tokenizer, length_params)\n",
    "    df_abstracts.at[index, 'Falcon'] = generated_text\n",
    "    df_abstracts.at[index, 'Falcon_len'] = [(word_count, sent_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV for log-ppx response calculation\n",
    "df_wiki.iloc[0:10].to_csv('src/wiki_dataset_lama3.csv', index=False)\n",
    "df_news.iloc[0:10].to_csv('src/news_dataset_lama3.csv', index=False)\n",
    "df_abstracts.iloc[0:10].to_csv('src/abstracts_dataset_lama3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
