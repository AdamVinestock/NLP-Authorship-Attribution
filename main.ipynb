{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from SentenceParser import SentenceParser\n",
    "# from PrepareSentenceContext import PrepareSentenceContext\n",
    "# import pandas as pd\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from PerplexityEvaluator import PerplexityEvaluator\n",
    "# from PrepareSentenceContext import PrepareSentenceContext\n",
    "# from tqdm import tqdm\n",
    "# from many_atomic_detections import generate_responses\n",
    "# from itertools import product\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from PerplexityEvaluator import PerplexityEvaluator\n",
    "from PrepareSentenceContext import PrepareSentenceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = ['src/wiki_dataset_clean.csv', 'src/news_dataset_clean.csv', 'src/abstract_dataset_clean.csv']\n",
    "# model_names = ['openai-community/gpt2', 'microsoft/Phi-3-mini-4k-instruct', 'tiiuae/falcon-7b']\n",
    "# context_policies = ['previous-3-sentences']\n",
    "\n",
    "model_names = ['openai-community/gpt2']\n",
    "context_policies = ['none']\n",
    "authors = ['human_text', 'Llama3.1_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ds_path, model_name, context_policy, author in product(dataset_paths, model_names, context_policies, authors):\n",
    "#     generate_responses(ds_path, model_name, context_policy, author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(\n",
    "    input_path, model_name, context_policy, author,\n",
    "    output_dir=\"Responses\", target_rows=100, checkpoint_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate responses for a dataset row by row, ensuring that the target number of rows from the dataset is processed.\n",
    "\n",
    "    Parameters:\n",
    "    - input_path: Path to the input dataset CSV.\n",
    "    - model_name: Name of the language model for perplexity evaluation.\n",
    "    - context_policy: Context policy to use for sentence processing.\n",
    "    - author_column: Column containing the text to process.\n",
    "    - output_dir: Directory where responses will be saved.\n",
    "    - target_rows: Number of rows from the dataset to process in this run.\n",
    "    - checkpoint_path: Path to save progress for crash recovery.\n",
    "    \"\"\"\n",
    "    # Load input dataset\n",
    "    df = pd.read_csv(input_path)\n",
    "    dataset_name = os.path.basename(input_path).split(\"_\")[0]\n",
    "\n",
    "    # Prepare output file path\n",
    "    if \"/\" in model_name:\n",
    "        lm_name_str = model_name.split(\"/\")[-1]\n",
    "    else:\n",
    "        lm_name_str = model_name\n",
    "    save_path = os.path.join(\n",
    "        output_dir, f\"{dataset_name}_{author}_{context_policy}_{lm_name_str}.csv\"\n",
    "    )\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load existing responses if file exists\n",
    "    if os.path.exists(save_path):\n",
    "        responses_df = pd.read_csv(save_path)\n",
    "    else:\n",
    "        responses_df = pd.DataFrame(columns=[\"num\", \"length\", \"response\", \"context_length\", \"name\"])\n",
    "\n",
    "    # Initialize model, tokenizer, and utilities\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    evaluator = PerplexityEvaluator(model, tokenizer)\n",
    "    parser = PrepareSentenceContext(context_policy=context_policy)\n",
    "\n",
    "    # Process rows individually\n",
    "    with tqdm(total=target_rows, desc=\"Processing rows\", unit=\"row\") as pbar:\n",
    "        for idx, row in df.iterrows():\n",
    "            if idx >= target_rows:\n",
    "                break\n",
    "\n",
    "            row_id = row.get(\"id\", idx)  # Use row index as fallback for ID\n",
    "\n",
    "            if row_id in responses_df[\"name\"].unique():\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            parsed = parser(row[author])\n",
    "\n",
    "            for sentence_num, (sentence, context) in enumerate(zip(parsed[\"text\"], parsed[\"context\"])):\n",
    "                try:\n",
    "                    response = evaluator(sentence, context)\n",
    "                    length = len(sentence.split())\n",
    "                    context_length = len(context.split()) if context else 0\n",
    "\n",
    "                    responses_df = pd.concat([\n",
    "                        responses_df,\n",
    "                        pd.DataFrame({\n",
    "                            \"num\": [sentence_num + 1],\n",
    "                            \"length\": [length],\n",
    "                            \"response\": [response],\n",
    "                            \"context_length\": [context_length],\n",
    "                            \"name\": [row_id]\n",
    "                        })\n",
    "                    ], ignore_index=True)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing sentence: {sentence[:50]} -> {e}\")\n",
    "\n",
    "            # Save progress incrementally\n",
    "            if checkpoint_path:\n",
    "                with open(checkpoint_path, \"w\") as f:\n",
    "                    f.write(str(idx + 1))\n",
    "            responses_df.to_csv(save_path, index=False)\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(f\"Finished processing rows. Saved to {save_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamvinestock/Documents/GitHub/NLP-Authorship-Attribution/env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3a314a976a4b3b91d8830f543cd251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/17.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615af485d11045fe9dfec80c1275872c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f61cfec3264408298e52ba0c72bd958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf5559aaefe74e5796a1ebc40bea7d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4521daca446e4ef78837f8437b947a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_responses(\n",
    "    input_path=\"src/wiki_dataset_clean.csv\",\n",
    "    model_name=\"tiiuae/falcon-7b\",\n",
    "    context_policy=\"none\",\n",
    "    author=\"human_text\",\n",
    "    target_rows=30\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
